[["index.html", "Data exploration and statistics with R and the tidyverse Chapter 1 Foreword", " Data exploration and statistics with R and the tidyverse Vivien Roussez (GoStudent) Winter 2022 Chapter 1 Foreword Welcome to this learning week in Asigmo’s program ! This week will be dedicated to data exploration, manipulation and visualization using R and the tidyverse. To that end, we will also cover the basics of statistics, that are essential for your understanding of modeling and, later on, machine learning. We will deal with complex and pretty dirty data, but this is real-world data, and you will also see that, more than artificial intelligence, you need common sense to deal with data ! This material was powered by bookdown To contact me : vivien.roussez@gmail.com "],["intro.html", "Chapter 2 Introduction 2.1 Topics of the week 2.2 The data science workflow 2.3 About the interactions with other colleagues 2.4 Resources 2.5 Data of the week 2.6 Your mission", " Chapter 2 Introduction 2.1 Topics of the week During this week, we will cover a lot of basic and nonetheless indispensable tools for a data scientist. We will cover mainly the “boring side” of data science (aka data analysis :P ). As a matter of fact, if machine learning is more and more automatized, everything that is related to data exploration, wrangling, cleaning, understanding and analysis is hardly doable by “AI.” What’s on our agenda : Introduction to R Data manipulation Introduction to statistics Data exploration Explainable machine learning (eg regression) Data visualization Data cleaning Dimension reduction Important notes : Of course, it’s not a linear path and the data science workflow is not a simple execution of those steps in a pre-determined order. It’s all connected and you’ll have to move back and forth between all of them to achieve your goal. And this goal, what is it already ? In data science is the word “science.” And what is science ? Huge topic… But a few keywords that you should always have in mind when starting a project Reproducibility Hypothesis testing Incremental Iterative Monitored 2.2 The data science workflow A very synthetic schematic of the data science workflow This process is a part of a large scheme : 2.3 About the interactions with other colleagues The previous iteration loop has to be done taking the business perspective / constraint into account ! More generally, the role of the data scientist is pretty central in use cases development. You will collaborate with other data/IT and business professionals : Data engineers Data architects Software engineers (DevOps) Machine learning engineers Data analysts Data strategists Program managers / product owners Business teams And one challenge is to have all those different profiles working for one common goal and communicate together. My experience is that the data scientist can really put some oil in the machinery, aand one of her/his duty is to bridge the gap between business and IT worlds. 2.4 Resources You will find a lot of resources online. Here is a selection, mostly related to R. However, our goal for this session is to have you understand how all the methods we will cover are related to each other and when you should consider using them R for data science Statistics and econometrics Statistics and exploration with R From data to viz (featuring python :D ) Data visualization More resource online books using R (text mining, machine learning…) 2.5 Data of the week In this course, we will handle with my own personal data (I give my consent ! ;) ) Those are my sports activity data, which I got from garmin (thanks to the GDPR). If you want to get your own data from this page. Being a triathlete, there are several sports involved and a lot of activities. The goal will be to import, clean, analyze the data with statistics and eventually build some first models (explainable AI). The export results in a lot of JSON files and we will focus on the summary data. It’s real world data and you’ll see it’s complicated, dirty and requires a lot of preparation/manipulation ! require(tidyverse) dat &lt;- read.csv(&quot;Data/Sports/Activities_2022.csv&quot;,header = T) glimpse(dat) ## Rows: 6,188 ## Columns: 88 ## $ activityId &lt;dbl&gt; 8117908979, 8112025942, 8111738850, 8106509382, 8101019939, 8100932442, 809484… ## $ uuidMsb &lt;dbl&gt; 1.389057e+18, -8.579595e+18, 4.389923e+18, -3.409299e+18, 8.398595e+18, -6.116… ## $ uuidLsb &lt;dbl&gt; -8.816745e+18, -6.087296e+18, -4.813906e+18, -6.907548e+18, -7.204024e+18, -8.… ## $ name &lt;chr&gt; &quot;Pool Swimming&quot;, &quot;Zwift - WTRL Team Time Trial - Zone 6 (C)&quot;, &quot;Zwift - .. *3.J… ## $ activityType &lt;chr&gt; &quot;lap_swimming&quot;, &quot;virtual_ride&quot;, &quot;virtual_ride&quot;, &quot;running&quot;, &quot;virtual_ride&quot;, &quot;vi… ## $ userProfileId &lt;int&gt; 1141258, 1141258, 1141258, 1141258, 1141258, 1141258, 1141258, 1141258, 114125… ## $ timeZoneId &lt;int&gt; 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124… ## $ beginTimestamp &lt;dbl&gt; 1.642184e+12, 1.642096e+12, 1.642094e+12, 1.642009e+12, 1.641928e+12, 1.641924… ## $ eventTypeId &lt;int&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, … ## $ rule &lt;chr&gt; &quot;public&quot;, &quot;public&quot;, &quot;public&quot;, &quot;public&quot;, &quot;public&quot;, &quot;public&quot;, &quot;public&quot;, &quot;public&quot;… ## $ sportType &lt;chr&gt; &quot;SWIMMING&quot;, &quot;CYCLING&quot;, &quot;CYCLING&quot;, &quot;RUNNING&quot;, &quot;CYCLING&quot;, &quot;CYCLING&quot;, &quot;CYCLING&quot;, … ## $ startTimeGmt &lt;dbl&gt; 1.642184e+12, 1.642096e+12, 1.642094e+12, 1.642009e+12, 1.641928e+12, 1.641924… ## $ startTimeLocal &lt;dbl&gt; 1.642188e+12, 1.642099e+12, 1.642097e+12, 1.642012e+12, 1.641932e+12, 1.641928… ## $ duration &lt;dbl&gt; 4271050, 2275000, 1707000, 4224043, 622000, 3954000, 4289000, 4297109, 2705840… ## $ distance &lt;dbl&gt; 340000, 2771270, 1640097, 1499398, 624256, 4927470, 5210229, 320000, 941099, 8… ## $ avgSpeed &lt;dbl&gt; 0.0993, 1.2181, 0.9608, 0.3550, 1.0036, 1.2461, 1.2147, 0.0942, 0.3478, 1.1559… ## $ avgHr &lt;int&gt; NA, 168, 135, 148, 138, 130, 158, NA, 150, 149, 141, 155, 149, 152, NA, 152, 1… ## $ maxHr &lt;int&gt; NA, 181, 154, 162, 151, 180, 173, NA, 172, 173, 167, 182, 168, 173, NA, 168, 1… ## $ avgPower &lt;int&gt; NA, 333, 240, NA, 186, 266, 270, NA, NA, 241, 239, NA, 280, 270, NA, NA, 228, … ## $ avgBikeCadence &lt;int&gt; NA, 86, 83, NA, 78, 84, 88, NA, NA, 85, 74, NA, 69, 88, NA, NA, 86, NA, 85, NA… ## $ maxBikeCadence &lt;int&gt; NA, 104, 200, NA, 100, 103, 120, NA, NA, 109, 114, NA, 108, 103, NA, NA, 109, … ## $ calories &lt;dbl&gt; 3033.5745, 3104.8048, 1638.2978, 3720.7378, 460.9022, 4290.5805, 4701.2024, 27… ## $ aerobicTrainingEffect &lt;dbl&gt; NA, NA, NA, 2.4, NA, NA, NA, NA, 2.7, NA, NA, 2.7, NA, NA, NA, 2.7, NA, 0.2, N… ## $ strokes &lt;int&gt; 1390, 0, 0, NA, 0, 0, 0, 1368, NA, 0, 0, NA, 0, 0, 916, NA, 0, NA, 0, NA, 0, N… ## $ normPower &lt;dbl&gt; NA, 348.5247, 249.8839, NA, 194.3214, 275.7212, 288.8615, NA, NA, 248.7237, 26… ## $ avgLeftBalance &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ avgRightBalance &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ max20MinPower &lt;dbl&gt; NA, 342.2592, 252.1425, NA, NA, 291.5917, 298.7442, NA, NA, 256.8575, 292.4200… ## $ avgFractionalCadence &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.1562500, 0.0000000, 0.0000000, 0.0000000, 0… ## $ maxFractionalCadence &lt;dbl&gt; 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0, 0.5… ## $ trainingStressScore &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ intensityFactor &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ elapsedDuration &lt;dbl&gt; 4685234, 2273000, 1706000, 4293429, 620000, 3953000, 4288000, 4505565, 2840719… ## $ movingDuration &lt;dbl&gt; 3466198, 2270000, 1706000, 4220701, 618000, 3950000, 4284000, 3427362, 2701710… ## $ anaerobicTrainingEffect &lt;dbl&gt; NA, NA, NA, 0.0, NA, NA, NA, NA, 0.0, NA, NA, 0.8, NA, NA, NA, 0.0, NA, 0.0, N… ## $ deviceId &lt;dbl&gt; 3968818126, 3825981698, 3825981698, 3968818126, 3825981698, 3825981698, 382598… ## $ minTemperature &lt;int&gt; 26, NA, NA, 7, NA, NA, NA, 26, 7, NA, NA, 13, NA, NA, 27, 14, NA, 24, NA, 17, … ## $ maxTemperature &lt;int&gt; 27, NA, NA, 24, NA, NA, NA, 26, 23, NA, NA, 25, NA, NA, 27, 26, NA, 27, NA, 26… ## $ lapCount &lt;int&gt; 39, 1, 1, 15, 1, 1, 1, 34, 10, 1, 1, 12, 1, 1, 26, 17, 1, 1, 1, 13, 1, 1, 12, … ## $ aerobicTrainingEffectMessage &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ anaerobicTrainingEffectMessage &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ purposeful &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ autoCalcCalories &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ favorite &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ pr &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ elevationCorrected &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ atpActivity &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ parent &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ elevationGain &lt;dbl&gt; NA, 32500, 7600, 16000, 3800, 26600, 26600, NA, 10400, 48900, 91800, 14300, 10… ## $ elevationLoss &lt;dbl&gt; NA, 0, 0, 18300, 0, 0, 0, NA, 10300, 0, 0, 15200, 0, 0, NA, 14400, 0, 0, 0, 15… ## $ maxSpeed &lt;dbl&gt; 0.3865, 1.9172, 1.2852, 0.4161, 1.6083, 1.7015, 1.7127, 0.1327, 0.3956, 1.6546… ## $ maxRunCadence &lt;int&gt; NA, NA, NA, 112, NA, NA, NA, NA, 94, NA, NA, 92, NA, NA, NA, 92, NA, NA, NA, 9… ## $ steps &lt;int&gt; NA, NA, NA, 12014, NA, NA, NA, NA, 7832, NA, NA, 9830, NA, NA, NA, 13782, NA, … ## $ startLongitude &lt;dbl&gt; NA, 0.000000, 0.000000, 16.315890, 0.000000, 0.000000, 0.000000, NA, 16.315754… ## $ startLatitude &lt;dbl&gt; NA, 0.00000, 0.00000, 48.21407, 0.00000, 0.00000, 0.00000, NA, 48.21414, 0.000… ## $ avgVerticalOscillation &lt;dbl&gt; NA, NA, NA, 8.26, NA, NA, NA, NA, 8.57, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ avgGroundContactTime &lt;dbl&gt; NA, NA, NA, 248.1, NA, NA, NA, NA, 236.8, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ avgStrideLength &lt;dbl&gt; NA, NA, NA, 118.2400, NA, NA, NA, NA, 116.2000, NA, NA, 121.2003, NA, NA, NA, … ## $ vO2MaxValue &lt;int&gt; NA, NA, NA, 60, NA, NA, NA, NA, 59, NA, NA, 60, NA, NA, NA, 59, NA, NA, NA, 60… ## $ avgVerticalRatio &lt;dbl&gt; NA, NA, NA, 6.83, NA, NA, NA, NA, 7.25, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ avgGroundContactBalance &lt;dbl&gt; NA, NA, NA, 48.93, NA, NA, NA, NA, 49.17, NA, NA, NA, NA, NA, NA, NA, NA, NA, … ## $ minElevation &lt;dbl&gt; NA, 120, 12180, 19480, 400, 1040, 1040, NA, 25160, -620, 27880, 8120, 720, 860… ## $ maxElevation &lt;dbl&gt; NA, 5780, 13040, 34900, 2900, 3400, 3400, NA, 34080, 2300, 69580, 15160, 10472… ## $ avgDoubleCadence &lt;dbl&gt; NA, NA, NA, 172.31250, NA, NA, NA, NA, 173.79688, NA, NA, 174.82812, NA, NA, N… ## $ maxDoubleCadence &lt;int&gt; NA, NA, NA, 225, NA, NA, NA, NA, 189, NA, NA, 185, NA, NA, NA, 185, NA, NA, NA… ## $ locationName &lt;chr&gt; NA, NA, NA, &quot;Vienna&quot;, &quot;Tower Hamlets&quot;, &quot;Thio&quot;, &quot;Thio&quot;, NA, &quot;Vienna&quot;, NA, &quot;Inns… ## $ maxVerticalSpeed &lt;dbl&gt; NA, 0.11999998, 0.05999985, 0.10000000, 0.06000004, 0.06000004, 0.06000004, NA… ## $ endLongitude &lt;dbl&gt; NA, 166.9529124, 165.8442626, 16.3033489, -0.1127895, 166.1724174, 166.1520484… ## $ endLatitude &lt;dbl&gt; NA, -11.63601, -10.79433, 48.21414, 51.49451, -21.73831, -21.74362, NA, 48.207… ## $ avgStrokes &lt;dbl&gt; 22.4, NA, NA, NA, NA, NA, NA, 23.6, NA, NA, NA, NA, NA, NA, 10.9, NA, NA, NA, … ## $ activeLengths &lt;int&gt; 68, NA, NA, NA, NA, NA, NA, 64, NA, NA, NA, NA, NA, NA, 84, NA, NA, NA, NA, NA… ## $ avgSwolf &lt;int&gt; 73, NA, NA, NA, NA, NA, NA, 77, NA, NA, NA, NA, NA, NA, 36, NA, NA, NA, NA, NA… ## $ poolLength &lt;dbl&gt; 5000, NA, NA, NA, NA, NA, NA, 5000, NA, NA, NA, NA, NA, NA, 2500, NA, NA, NA, … ## $ avgStrokeDistance &lt;dbl&gt; 223, NA, NA, NA, NA, NA, NA, 212, NA, NA, NA, NA, NA, NA, 229, NA, NA, NA, NA,… ## $ avgSwimCadence &lt;int&gt; 27, NA, NA, NA, NA, NA, NA, 27, NA, NA, NA, NA, NA, NA, 26, NA, NA, NA, NA, NA… ## $ maxSwimCadence &lt;int&gt; 31, NA, NA, NA, NA, NA, NA, 30, NA, NA, NA, NA, NA, NA, 29, NA, NA, NA, NA, NA… ## $ workoutId &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ activeSets &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ totalSets &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ totalReps &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ parentId &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ manufacturer &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ courseId &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ maxFtp &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ avgVerticalSpeed &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ decoDive &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ lactateThresholdBpm &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ lactateThresholdSpeed &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… 2.6 Your mission The data we will use as an example is quite messy, but it is a rather traditional tye of data that we will handle. However, you will face diverse type of data : time series, graphs, geospatial, panel,… That will require different analytical tools but also different ways to handle those objects. To provide you with a taste of those challenges ahead, your mission will be the following : Select the most important metrics of the activities (distance, duration, speed, power, elevation, heart rate….) and create monthly time series out of it (you will have to aggregate it ; please choose wisely how to aggregate : simple average can be inaccurate for ratios). Pick 4 of them. Learn by yourself the basics of time series and please describe for the chosen time series : Draw the autocorrelation function Is there a trend ? Is there a seasonal pattern ? Are those series stationary ? With which level of confidence can you assert so ? Please use 2 different tests. Prediction is the most common ML task for time series. Try the prophet and auto.arima algorithms to predict each time series 12 months ahead. What can you conclude ? "],["intro_r.html", "Chapter 3 Introduction to R 3.1 What is R 3.2 Basic commands to know 3.3 Data structures in R", " Chapter 3 Introduction to R R is one of the most popular data science language along with Python and Julia 3.1 What is R 3.1.1 Description R is an open source programming language initially dedicated to statistics and data analysis. It is the open-source version of the original S/S-plus language, developed by Bell labs a looong time ago. It was developed in the late 90’s. Being open-source, the number of packages available is considerable, generating both completeness and confusion. R is a functional programming language, meaning that functions are at the very core of its usage. It is not a object-oriented language although there are classes, but which are mainly hidden from the end-user. THe basic R is a command line interface, pretty similar to the bash It is an interactive language, meaning you can execute commands one after the other, no compilation is needed to execute a sequence of commands and you can try / adjust yourt code on the fly \\(\\rightarrow\\) very flexible (like IPython) Another very interesting characteristic of the language is that it is by design vectorized, meaning operations are executed at once on vectors, without explicit loops, which makes it very effective (as long as you don’t loop…) 3.1.2 (Objective) comparison with Pyhton What they have in common : Both language are open source and come with a wide set of capabilities and a community Interactivity Data science development environment (Jupyter) Several ways to achieve the same task What differs : R is dedicated to data / python is a generic programming language Functional vs object oriented Analysis (R) vs final product (Py) orientation 3.1.3 What can I do with R R’s core relies in data manipulation and statistical analysis. But the community made it grow in many directions Read data from multiple sources (excel, text files, databases, big data infrastructures…) Machine learning and deep learning Data visualization Communication Publications … Usages I probably have no idea about !! 3.1.4 Quick presentation of the ecosystem The core functionalities are available with base R on CRAN. On top of that, you can install several IDEs, the most popular ones being Rstudio, Jupyter or VSCode For this training, we will use the R kernel of google collabs, but for many purposes, you’ll have to use another IDE (shiny, markdown…). This kernel comes with basics packages AND the tidyverse To add features to R, you’ll have then to install packages. Generally, when facing a problem (eg : I have to implement a naive Bayes estimator), you google it adding r at the beginning of the query and you’ll get the name of the packages that allow you to do that. Then you can install and activate it. # install.packages(&quot;e1071&quot;) library(e1071) Note : You can also call functions from an installed package without loading the whole package with ::. You might prefer this solution in several cases : You use only one function from the package only once \\(\\rightarrow\\) maybe not necessary to load everything Function names can be common across packages (eg: intersect, summarise…) using :: ensures you are using the function from the package you meant. Drawback : when not appearing at the beginning of the script, it can be unseen (for a new user) that the script requires such package to be installed dplyr::summarise(iris,n_species=dplyr::n_distinct(Species)) ## n_species ## 1 3 To find more information about R and its functinalities / latest news : R bloggers tidyverse.org twitter : #rstat Rstudio website Now you can use all functions of this packages ! 3.2 Basic commands to know Where to find help : Search engine to know how to do something help(lm) or ?lm to get help about a specific function (its inputs and output) stackoverflow to debug List the objects in memory ls() What is the current directory getwd() ; change it setwd() Browse folders and files dir() Session information (loaded packages and so on) sessionInfo() Install and load packages : see above View the source code of a function : lm Create a new object and assign a value to it &lt;-. display in the console by typing its name Commented lines, like in Python, start with a # ?lm ls() ## [1] &quot;acp&quot; &quot;acp_dat&quot; &quot;agg&quot; ## [4] &quot;all_fields&quot; &quot;anova&quot; &quot;arr&quot; ## [7] &quot;bike&quot; &quot;char&quot; &quot;dat&quot; ## [10] &quot;dat_bike&quot; &quot;dat_bike_imp&quot; &quot;dat_clean&quot; ## [13] &quot;dat_long&quot; &quot;dd&quot; &quot;ex_dep&quot; ## [16] &quot;ex_indep&quot; &quot;extract_field_from_single_act&quot; &quot;extract_from_all_activities&quot; ## [19] &quot;fake&quot; &quot;gg1&quot; &quot;gg2&quot; ## [22] &quot;imports&quot; &quot;logit&quot; &quot;long_data&quot; ## [25] &quot;mat&quot; &quot;missing&quot; &quot;most_freq_cat&quot; ## [28] &quot;mu&quot; &quot;my_list&quot; &quot;my_vect&quot; ## [31] &quot;n&quot; &quot;non_bike&quot; &quot;obj&quot; ## [34] &quot;pred&quot; &quot;pred_bin&quot; &quot;random_list&quot; ## [37] &quot;reg&quot; &quot;reg_full&quot; &quot;reshaped&quot; ## [40] &quot;rev_char&quot; &quot;selection&quot; &quot;series&quot; ## [43] &quot;sigma&quot; &quot;sim&quot; &quot;sports&quot; ## [46] &quot;square&quot; &quot;stats&quot; &quot;sub3&quot; ## [49] &quot;tab&quot; &quot;test&quot; &quot;test_and_extract&quot; ## [52] &quot;tsne&quot; &quot;Xt&quot; sessionInfo() ## R version 4.1.1 (2021-08-10) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Monterey 12.1 ## ## Matrix products: default ## LAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] plotly_4.10.0 Rtsne_0.15 FactoMineR_2.4 GGally_2.1.2 ggpubr_0.4.0 tseries_0.10-49 e1071_1.7-9 ## [8] jsonlite_1.7.3 forcats_0.5.1 stringr_1.4.0 dplyr_1.0.7 purrr_0.3.4 readr_2.1.1 tidyr_1.1.4 ## [15] tibble_3.1.6 ggplot2_3.3.5 tidyverse_1.3.1 lubridate_1.8.0 ## ## loaded via a namespace (and not attached): ## [1] nlme_3.1-152 fs_1.5.2 xts_0.12.1 progress_1.2.2 RColorBrewer_1.1-2 ## [6] httr_1.4.2 tools_4.1.1 backports_1.4.1 DT_0.20 utf8_1.2.2 ## [11] R6_2.5.1 lazyeval_0.2.2 DBI_1.1.2 mgcv_1.8-36 colorspace_2.0-2 ## [16] withr_2.4.3 prettyunits_1.1.1 tidyselect_1.1.1 curl_4.3.2 compiler_4.1.1 ## [21] cli_3.1.1 lsr_0.5.2 rvest_1.0.2 flashClust_1.01-2 xml2_1.3.3 ## [26] labeling_0.4.2 bookdown_0.24 scales_1.1.1 quadprog_1.5-8 proxy_0.4-26 ## [31] digest_0.6.29 rmarkdown_2.11 pkgconfig_2.0.3 htmltools_0.5.2 dbplyr_2.1.1 ## [36] fastmap_1.1.0 highr_0.9 htmlwidgets_1.5.4 rlang_0.4.12 readxl_1.3.1 ## [41] TTR_0.24.3 rstudioapi_0.13 quantmod_0.4.18 jquerylib_0.1.4 farver_2.1.0 ## [46] generics_0.1.1 zoo_1.8-9 car_3.0-12 magrittr_2.0.1 leaps_3.1 ## [51] Matrix_1.3-4 Rcpp_1.0.8 munsell_0.5.0 fansi_1.0.2 abind_1.4-5 ## [56] lifecycle_1.0.1 scatterplot3d_0.3-41 stringi_1.7.6 yaml_2.2.1 carData_3.0-5 ## [61] MASS_7.3-54 plyr_1.8.6 grid_4.1.1 ggrepel_0.9.1 crayon_1.4.2 ## [66] lattice_0.20-44 haven_2.4.3 cowplot_1.1.1 splines_4.1.1 hms_1.1.1 ## [71] knitr_1.37 pillar_1.6.4 ggsignif_0.6.3 reprex_2.0.1 glue_1.6.1 ## [76] evaluate_0.14 data.table_1.14.2 modelr_0.1.8 vctrs_0.3.8 tzdb_0.2.0 ## [81] cellranger_1.1.0 gtable_0.3.0 reshape_0.8.8 assertthat_0.2.1 xfun_0.29 ## [86] broom_0.7.11 rstatix_0.7.0 viridisLite_0.4.0 class_7.3-19 cluster_2.1.2 ## [91] ellipsis_0.3.2 getwd() ## [1] &quot;/Users/vivienroussez/Documents/Datascience/Asigmo/DataExploration&quot; dir(&quot;/&quot;) ## [1] &quot;Applications&quot; &quot;bin&quot; &quot;cores&quot; &quot;dev&quot; &quot;etc&quot; &quot;home&quot; &quot;Library&quot; ## [8] &quot;opt&quot; &quot;private&quot; &quot;sbin&quot; &quot;System&quot; &quot;tmp&quot; &quot;Users&quot; &quot;usr&quot; ## [15] &quot;var&quot; &quot;Volumes&quot; lm ## function (formula, data, subset, weights, na.action, method = &quot;qr&quot;, ## model = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, ## contrasts = NULL, offset, ...) ## { ## ret.x &lt;- x ## ret.y &lt;- y ## cl &lt;- match.call() ## mf &lt;- match.call(expand.dots = FALSE) ## m &lt;- match(c(&quot;formula&quot;, &quot;data&quot;, &quot;subset&quot;, &quot;weights&quot;, &quot;na.action&quot;, ## &quot;offset&quot;), names(mf), 0L) ## mf &lt;- mf[c(1L, m)] ## mf$drop.unused.levels &lt;- TRUE ## mf[[1L]] &lt;- quote(stats::model.frame) ## mf &lt;- eval(mf, parent.frame()) ## if (method == &quot;model.frame&quot;) ## return(mf) ## else if (method != &quot;qr&quot;) ## warning(gettextf(&quot;method = &#39;%s&#39; is not supported. Using &#39;qr&#39;&quot;, ## method), domain = NA) ## mt &lt;- attr(mf, &quot;terms&quot;) ## y &lt;- model.response(mf, &quot;numeric&quot;) ## w &lt;- as.vector(model.weights(mf)) ## if (!is.null(w) &amp;&amp; !is.numeric(w)) ## stop(&quot;&#39;weights&#39; must be a numeric vector&quot;) ## offset &lt;- model.offset(mf) ## mlm &lt;- is.matrix(y) ## ny &lt;- if (mlm) ## nrow(y) ## else length(y) ## if (!is.null(offset)) { ## if (!mlm) ## offset &lt;- as.vector(offset) ## if (NROW(offset) != ny) ## stop(gettextf(&quot;number of offsets is %d, should equal %d (number of observations)&quot;, ## NROW(offset), ny), domain = NA) ## } ## if (is.empty.model(mt)) { ## x &lt;- NULL ## z &lt;- list(coefficients = if (mlm) matrix(NA_real_, 0, ## ncol(y)) else numeric(), residuals = y, fitted.values = 0 * ## y, weights = w, rank = 0L, df.residual = if (!is.null(w)) sum(w != ## 0) else ny) ## if (!is.null(offset)) { ## z$fitted.values &lt;- offset ## z$residuals &lt;- y - offset ## } ## } ## else { ## x &lt;- model.matrix(mt, mf, contrasts) ## z &lt;- if (is.null(w)) ## lm.fit(x, y, offset = offset, singular.ok = singular.ok, ## ...) ## else lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, ## ...) ## } ## class(z) &lt;- c(if (mlm) &quot;mlm&quot;, &quot;lm&quot;) ## z$na.action &lt;- attr(mf, &quot;na.action&quot;) ## z$offset &lt;- offset ## z$contrasts &lt;- attr(x, &quot;contrasts&quot;) ## z$xlevels &lt;- .getXlevels(mt, mf) ## z$call &lt;- cl ## z$terms &lt;- mt ## if (model) ## z$model &lt;- mf ## if (ret.x) ## z$x &lt;- x ## if (ret.y) ## z$y &lt;- y ## if (!qr) ## z$qr &lt;- NULL ## z ## } ## &lt;bytecode: 0x7fe9f6f55698&gt; ## &lt;environment: namespace:stats&gt; obj &lt;- 3 obj ## [1] 3 3.3 Data structures in R As mentioned, R is a functional programming language, which means that you will always call… functions. And functions are defined by parameters : the inputs you have to provide the function so that it can do what it’s meant for the result : the output you get. Stricly speacking, the result of a function is unique (as opposed to procedures). Of course, depending on the class of the result, it may of course be composite This chapter gives you some keys to understand and explore the results as they are provided by the functions. 3.3.1 Basic data structures Before introducing the data structure, a short precision about types. Values are stored in data structures which partially depend on their type : Logical (TRUE or FALSE) Numerical (integer, continuous or complex) Character (strings or categories) R recognizes the type of the value and modify it dynamically (no need to declare the type and it can e changed). To force R to coerce values to another type, you can use the functions as.numeric, as.character, as.logical. Important note : NA stands for not available and is common to all type when a value is missing. You can have other missing values though for numerical variables : Nan (not a mumber) eg 0/0 Inf (infinity) eg log(0) Attention : NULL applies to objects (eg a matrix or a list) and not to values themselves 3.3.1.1 Vectors Vectors are the basic data structure : it is a unidimensional collection of values having the same type. There are a lot of ways to generate vectors : my_vect &lt;- c(1,2,19,1) ; print(my_vect) ## [1] 1 2 19 1 my_vect &lt;- 1:10 ; print(my_vect) ## [1] 1 2 3 4 5 6 7 8 9 10 my_vect &lt;- seq(-15,100,.1) ; print(my_vect) ## [1] -15.0 -14.9 -14.8 -14.7 -14.6 -14.5 -14.4 -14.3 -14.2 -14.1 -14.0 -13.9 -13.8 -13.7 -13.6 -13.5 -13.4 -13.3 ## [19] -13.2 -13.1 -13.0 -12.9 -12.8 -12.7 -12.6 -12.5 -12.4 -12.3 -12.2 -12.1 -12.0 -11.9 -11.8 -11.7 -11.6 -11.5 ## [37] -11.4 -11.3 -11.2 -11.1 -11.0 -10.9 -10.8 -10.7 -10.6 -10.5 -10.4 -10.3 -10.2 -10.1 -10.0 -9.9 -9.8 -9.7 ## [55] -9.6 -9.5 -9.4 -9.3 -9.2 -9.1 -9.0 -8.9 -8.8 -8.7 -8.6 -8.5 -8.4 -8.3 -8.2 -8.1 -8.0 -7.9 ## [73] -7.8 -7.7 -7.6 -7.5 -7.4 -7.3 -7.2 -7.1 -7.0 -6.9 -6.8 -6.7 -6.6 -6.5 -6.4 -6.3 -6.2 -6.1 ## [91] -6.0 -5.9 -5.8 -5.7 -5.6 -5.5 -5.4 -5.3 -5.2 -5.1 -5.0 -4.9 -4.8 -4.7 -4.6 -4.5 -4.4 -4.3 ## [109] -4.2 -4.1 -4.0 -3.9 -3.8 -3.7 -3.6 -3.5 -3.4 -3.3 -3.2 -3.1 -3.0 -2.9 -2.8 -2.7 -2.6 -2.5 ## [127] -2.4 -2.3 -2.2 -2.1 -2.0 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 -1.0 -0.9 -0.8 -0.7 ## [145] -0.6 -0.5 -0.4 -0.3 -0.2 -0.1 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 ## [163] 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 2.4 2.5 2.6 2.7 2.8 2.9 ## [181] 3.0 3.1 3.2 3.3 3.4 3.5 3.6 3.7 3.8 3.9 4.0 4.1 4.2 4.3 4.4 4.5 4.6 4.7 ## [199] 4.8 4.9 5.0 5.1 5.2 5.3 5.4 5.5 5.6 5.7 5.8 5.9 6.0 6.1 6.2 6.3 6.4 6.5 ## [217] 6.6 6.7 6.8 6.9 7.0 7.1 7.2 7.3 7.4 7.5 7.6 7.7 7.8 7.9 8.0 8.1 8.2 8.3 ## [235] 8.4 8.5 8.6 8.7 8.8 8.9 9.0 9.1 9.2 9.3 9.4 9.5 9.6 9.7 9.8 9.9 10.0 10.1 ## [253] 10.2 10.3 10.4 10.5 10.6 10.7 10.8 10.9 11.0 11.1 11.2 11.3 11.4 11.5 11.6 11.7 11.8 11.9 ## [271] 12.0 12.1 12.2 12.3 12.4 12.5 12.6 12.7 12.8 12.9 13.0 13.1 13.2 13.3 13.4 13.5 13.6 13.7 ## [289] 13.8 13.9 14.0 14.1 14.2 14.3 14.4 14.5 14.6 14.7 14.8 14.9 15.0 15.1 15.2 15.3 15.4 15.5 ## [307] 15.6 15.7 15.8 15.9 16.0 16.1 16.2 16.3 16.4 16.5 16.6 16.7 16.8 16.9 17.0 17.1 17.2 17.3 ## [325] 17.4 17.5 17.6 17.7 17.8 17.9 18.0 18.1 18.2 18.3 18.4 18.5 18.6 18.7 18.8 18.9 19.0 19.1 ## [343] 19.2 19.3 19.4 19.5 19.6 19.7 19.8 19.9 20.0 20.1 20.2 20.3 20.4 20.5 20.6 20.7 20.8 20.9 ## [361] 21.0 21.1 21.2 21.3 21.4 21.5 21.6 21.7 21.8 21.9 22.0 22.1 22.2 22.3 22.4 22.5 22.6 22.7 ## [379] 22.8 22.9 23.0 23.1 23.2 23.3 23.4 23.5 23.6 23.7 23.8 23.9 24.0 24.1 24.2 24.3 24.4 24.5 ## [397] 24.6 24.7 24.8 24.9 25.0 25.1 25.2 25.3 25.4 25.5 25.6 25.7 25.8 25.9 26.0 26.1 26.2 26.3 ## [415] 26.4 26.5 26.6 26.7 26.8 26.9 27.0 27.1 27.2 27.3 27.4 27.5 27.6 27.7 27.8 27.9 28.0 28.1 ## [433] 28.2 28.3 28.4 28.5 28.6 28.7 28.8 28.9 29.0 29.1 29.2 29.3 29.4 29.5 29.6 29.7 29.8 29.9 ## [451] 30.0 30.1 30.2 30.3 30.4 30.5 30.6 30.7 30.8 30.9 31.0 31.1 31.2 31.3 31.4 31.5 31.6 31.7 ## [469] 31.8 31.9 32.0 32.1 32.2 32.3 32.4 32.5 32.6 32.7 32.8 32.9 33.0 33.1 33.2 33.3 33.4 33.5 ## [487] 33.6 33.7 33.8 33.9 34.0 34.1 34.2 34.3 34.4 34.5 34.6 34.7 34.8 34.9 35.0 35.1 35.2 35.3 ## [505] 35.4 35.5 35.6 35.7 35.8 35.9 36.0 36.1 36.2 36.3 36.4 36.5 36.6 36.7 36.8 36.9 37.0 37.1 ## [523] 37.2 37.3 37.4 37.5 37.6 37.7 37.8 37.9 38.0 38.1 38.2 38.3 38.4 38.5 38.6 38.7 38.8 38.9 ## [541] 39.0 39.1 39.2 39.3 39.4 39.5 39.6 39.7 39.8 39.9 40.0 40.1 40.2 40.3 40.4 40.5 40.6 40.7 ## [559] 40.8 40.9 41.0 41.1 41.2 41.3 41.4 41.5 41.6 41.7 41.8 41.9 42.0 42.1 42.2 42.3 42.4 42.5 ## [577] 42.6 42.7 42.8 42.9 43.0 43.1 43.2 43.3 43.4 43.5 43.6 43.7 43.8 43.9 44.0 44.1 44.2 44.3 ## [595] 44.4 44.5 44.6 44.7 44.8 44.9 45.0 45.1 45.2 45.3 45.4 45.5 45.6 45.7 45.8 45.9 46.0 46.1 ## [613] 46.2 46.3 46.4 46.5 46.6 46.7 46.8 46.9 47.0 47.1 47.2 47.3 47.4 47.5 47.6 47.7 47.8 47.9 ## [631] 48.0 48.1 48.2 48.3 48.4 48.5 48.6 48.7 48.8 48.9 49.0 49.1 49.2 49.3 49.4 49.5 49.6 49.7 ## [649] 49.8 49.9 50.0 50.1 50.2 50.3 50.4 50.5 50.6 50.7 50.8 50.9 51.0 51.1 51.2 51.3 51.4 51.5 ## [667] 51.6 51.7 51.8 51.9 52.0 52.1 52.2 52.3 52.4 52.5 52.6 52.7 52.8 52.9 53.0 53.1 53.2 53.3 ## [685] 53.4 53.5 53.6 53.7 53.8 53.9 54.0 54.1 54.2 54.3 54.4 54.5 54.6 54.7 54.8 54.9 55.0 55.1 ## [703] 55.2 55.3 55.4 55.5 55.6 55.7 55.8 55.9 56.0 56.1 56.2 56.3 56.4 56.5 56.6 56.7 56.8 56.9 ## [721] 57.0 57.1 57.2 57.3 57.4 57.5 57.6 57.7 57.8 57.9 58.0 58.1 58.2 58.3 58.4 58.5 58.6 58.7 ## [739] 58.8 58.9 59.0 59.1 59.2 59.3 59.4 59.5 59.6 59.7 59.8 59.9 60.0 60.1 60.2 60.3 60.4 60.5 ## [757] 60.6 60.7 60.8 60.9 61.0 61.1 61.2 61.3 61.4 61.5 61.6 61.7 61.8 61.9 62.0 62.1 62.2 62.3 ## [775] 62.4 62.5 62.6 62.7 62.8 62.9 63.0 63.1 63.2 63.3 63.4 63.5 63.6 63.7 63.8 63.9 64.0 64.1 ## [793] 64.2 64.3 64.4 64.5 64.6 64.7 64.8 64.9 65.0 65.1 65.2 65.3 65.4 65.5 65.6 65.7 65.8 65.9 ## [811] 66.0 66.1 66.2 66.3 66.4 66.5 66.6 66.7 66.8 66.9 67.0 67.1 67.2 67.3 67.4 67.5 67.6 67.7 ## [829] 67.8 67.9 68.0 68.1 68.2 68.3 68.4 68.5 68.6 68.7 68.8 68.9 69.0 69.1 69.2 69.3 69.4 69.5 ## [847] 69.6 69.7 69.8 69.9 70.0 70.1 70.2 70.3 70.4 70.5 70.6 70.7 70.8 70.9 71.0 71.1 71.2 71.3 ## [865] 71.4 71.5 71.6 71.7 71.8 71.9 72.0 72.1 72.2 72.3 72.4 72.5 72.6 72.7 72.8 72.9 73.0 73.1 ## [883] 73.2 73.3 73.4 73.5 73.6 73.7 73.8 73.9 74.0 74.1 74.2 74.3 74.4 74.5 74.6 74.7 74.8 74.9 ## [901] 75.0 75.1 75.2 75.3 75.4 75.5 75.6 75.7 75.8 75.9 76.0 76.1 76.2 76.3 76.4 76.5 76.6 76.7 ## [919] 76.8 76.9 77.0 77.1 77.2 77.3 77.4 77.5 77.6 77.7 77.8 77.9 78.0 78.1 78.2 78.3 78.4 78.5 ## [937] 78.6 78.7 78.8 78.9 79.0 79.1 79.2 79.3 79.4 79.5 79.6 79.7 79.8 79.9 80.0 80.1 80.2 80.3 ## [955] 80.4 80.5 80.6 80.7 80.8 80.9 81.0 81.1 81.2 81.3 81.4 81.5 81.6 81.7 81.8 81.9 82.0 82.1 ## [973] 82.2 82.3 82.4 82.5 82.6 82.7 82.8 82.9 83.0 83.1 83.2 83.3 83.4 83.5 83.6 83.7 83.8 83.9 ## [991] 84.0 84.1 84.2 84.3 84.4 84.5 84.6 84.7 84.8 84.9 ## [ reached getOption(&quot;max.print&quot;) -- omitted 151 entries ] my_vect &lt;- rnorm(100) ; print(my_vect) ## [1] -0.200223818 -1.291224473 -0.284059695 -1.866971704 -0.224491615 0.426022265 -0.147040597 0.155690683 ## [9] -0.767850975 -0.164243861 0.005750509 0.188510620 1.304034918 0.981109427 1.744008415 -0.375137106 ## [17] -0.512823580 -1.523444623 1.434225662 0.151598404 0.796753119 0.150822727 0.577223391 0.343459717 ## [25] 0.683979107 -0.281168656 -0.072335741 0.590593545 -0.952107454 -1.627610863 -0.436591725 -0.730098736 ## [33] 0.908888545 0.501050435 -1.511430336 1.320382510 0.714083532 -0.271877892 -0.692632907 0.115353658 ## [41] -0.205499825 0.868850488 0.728680874 0.401298551 0.169343133 -1.194532541 0.437593134 0.276659620 ## [49] -0.069179826 -0.445105406 -0.721632141 0.824819926 0.359722225 0.627006951 0.082222936 1.498750327 ## [57] 0.319514263 -0.870614338 0.101278949 -0.380320197 2.503310005 -0.020087165 2.050646818 0.463702269 ## [65] 0.747615320 -0.827696971 0.544555965 -0.283187143 0.663374403 -1.384153136 -0.073250966 0.362790970 ## [73] -0.842116263 -0.009856304 -1.661308426 -0.273717410 -0.248722940 -1.325389139 -0.292745328 -0.909912486 ## [81] -0.860632862 0.390578524 0.135081707 0.777427540 0.588388435 -0.367789539 2.108718793 1.760833845 ## [89] -1.679842839 0.472096017 0.277539568 1.303726895 -0.753970593 0.739566348 -0.222720609 -0.789630775 ## [97] 0.560649399 -0.483649621 -2.040980386 0.162548826 my_vect &lt;- sample(letters,100,replace = T) ; print(my_vect) ## [1] &quot;m&quot; &quot;l&quot; &quot;k&quot; &quot;g&quot; &quot;f&quot; &quot;n&quot; &quot;k&quot; &quot;h&quot; &quot;r&quot; &quot;e&quot; &quot;e&quot; &quot;b&quot; &quot;u&quot; &quot;t&quot; &quot;d&quot; &quot;p&quot; &quot;k&quot; &quot;r&quot; &quot;l&quot; &quot;s&quot; &quot;t&quot; &quot;g&quot; &quot;h&quot; &quot;s&quot; &quot;j&quot; &quot;m&quot; &quot;e&quot; &quot;c&quot; ## [29] &quot;d&quot; &quot;g&quot; &quot;r&quot; &quot;g&quot; &quot;o&quot; &quot;a&quot; &quot;o&quot; &quot;c&quot; &quot;h&quot; &quot;t&quot; &quot;k&quot; &quot;p&quot; &quot;o&quot; &quot;e&quot; &quot;w&quot; &quot;c&quot; &quot;r&quot; &quot;u&quot; &quot;w&quot; &quot;a&quot; &quot;v&quot; &quot;m&quot; &quot;l&quot; &quot;x&quot; &quot;p&quot; &quot;d&quot; &quot;r&quot; &quot;g&quot; ## [57] &quot;s&quot; &quot;x&quot; &quot;m&quot; &quot;z&quot; &quot;g&quot; &quot;k&quot; &quot;l&quot; &quot;a&quot; &quot;n&quot; &quot;d&quot; &quot;m&quot; &quot;k&quot; &quot;p&quot; &quot;r&quot; &quot;k&quot; &quot;a&quot; &quot;s&quot; &quot;r&quot; &quot;j&quot; &quot;y&quot; &quot;x&quot; &quot;p&quot; &quot;h&quot; &quot;t&quot; &quot;i&quot; &quot;s&quot; &quot;a&quot; &quot;p&quot; ## [85] &quot;u&quot; &quot;x&quot; &quot;l&quot; &quot;k&quot; &quot;s&quot; &quot;u&quot; &quot;j&quot; &quot;w&quot; &quot;w&quot; &quot;f&quot; &quot;v&quot; &quot;u&quot; &quot;p&quot; &quot;s&quot; &quot;m&quot; &quot;v&quot; You can access vector values with integer indexes (that are vector themselves). Note : unlike Python, the indexes start with the value 1, not 0 ! my_vect[4] ## [1] &quot;g&quot; my_vect[1:4] ## [1] &quot;m&quot; &quot;l&quot; &quot;k&quot; &quot;g&quot; A vector can be named meaning that each element has a name through which it can be accessed. my_vect &lt;- 1:10 names(my_vect) &lt;- letters[1:10] ; print(my_vect) ## a b c d e f g h i j ## 1 2 3 4 5 6 7 8 9 10 my_vect[&quot;b&quot;] ## b ## 2 Did you notice you can assign values to a vector’s attribute ? :D 3.3.1.2 Matrices and arrays Matrices are a 2-dimensional collection of values having the same type. An array is an extension of matrices for more than 2 dimensions. mat &lt;- matrix(1,ncol=10,nrow = 15) ; print(mat) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 1 1 1 1 1 1 1 1 1 ## [2,] 1 1 1 1 1 1 1 1 1 1 ## [3,] 1 1 1 1 1 1 1 1 1 1 ## [4,] 1 1 1 1 1 1 1 1 1 1 ## [5,] 1 1 1 1 1 1 1 1 1 1 ## [6,] 1 1 1 1 1 1 1 1 1 1 ## [7,] 1 1 1 1 1 1 1 1 1 1 ## [8,] 1 1 1 1 1 1 1 1 1 1 ## [9,] 1 1 1 1 1 1 1 1 1 1 ## [10,] 1 1 1 1 1 1 1 1 1 1 ## [11,] 1 1 1 1 1 1 1 1 1 1 ## [12,] 1 1 1 1 1 1 1 1 1 1 ## [13,] 1 1 1 1 1 1 1 1 1 1 ## [14,] 1 1 1 1 1 1 1 1 1 1 ## [15,] 1 1 1 1 1 1 1 1 1 1 mat &lt;- matrix(1:5,ncol=5,nrow=7) ; print(mat) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 2 4 ## [2,] 2 4 1 3 5 ## [3,] 3 5 2 4 1 ## [4,] 4 1 3 5 2 ## [5,] 5 2 4 1 3 ## [6,] 1 3 5 2 4 ## [7,] 2 4 1 3 5 arr &lt;- array(1:10,dim = c(10,2,3)) ; print(arr) ## , , 1 ## ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 ## [4,] 4 4 ## [5,] 5 5 ## [6,] 6 6 ## [7,] 7 7 ## [8,] 8 8 ## [9,] 9 9 ## [10,] 10 10 ## ## , , 2 ## ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 ## [4,] 4 4 ## [5,] 5 5 ## [6,] 6 6 ## [7,] 7 7 ## [8,] 8 8 ## [9,] 9 9 ## [10,] 10 10 ## ## , , 3 ## ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 ## [4,] 4 4 ## [5,] 5 5 ## [6,] 6 6 ## [7,] 7 7 ## [8,] 8 8 ## [9,] 9 9 ## [10,] 10 10 matrix(rnorm(9),3,3) m1 &lt;- matrix(1,2,3) m2 &lt;- matrix(1,3,2) m1*m2 3.3.1.3 Lists Lists are a very versatile and convenient class that allows you to store heterogeneous values and data structures my_list &lt;- list(&quot;A&quot;,1,LETTERS[1:10],matrix(1,3,3)) ; my_list ## [[1]] ## [1] &quot;A&quot; ## ## [[2]] ## [1] 1 ## ## [[3]] ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; ## ## [[4]] ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 1 1 1 ## [3,] 1 1 1 Like with vectors, list elements can be accessed via their index or their name. If a list has been named, you have something very similar to python dictionaries. In case the list is named, you can also access its elements via the $ operator. names(my_list) &lt;- paste0(&quot;thing&quot;,1:length(my_list)) my_list[1] ## $thing1 ## [1] &quot;A&quot; my_list[&quot;thing1&quot;] ## $thing1 ## [1] &quot;A&quot; my_list$thing3 ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; # my_list[2]*10 3.3.1.4 Dataframes A Dataframe is the most common data representation (think of an excel spreadsheet): it is made out of columns and rows like a matrix, but the columns can have different types. In R, Dataframes are natives (no need to install another package). They are basically a list of vectors that have the same length. Let’s have a look at Fisher’s iris dataframe (included in base R for demonstration purposes) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa to explore the content of a dataframe, you can of course print it, but if you want amore detailed overview of it, you can use the str or the glimpse functions str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... glimpse(iris) ## Rows: 150 ## Columns: 5 ## $ Sepal.Length &lt;dbl&gt; 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.9, 5.4, 4.8, 4.8, 4.3, 5.8, 5.7, 5.4, 5.1, 5.7, 5… ## $ Sepal.Width &lt;dbl&gt; 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.1, 3.7, 3.4, 3.0, 3.0, 4.0, 4.4, 3.9, 3.5, 3.8, 3… ## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.5, 1.5, 1.6, 1.4, 1.1, 1.2, 1.5, 1.3, 1.4, 1.7, 1… ## $ Petal.Width &lt;dbl&gt; 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.2, 0.2, 0.1, 0.1, 0.2, 0.4, 0.4, 0.3, 0.3, 0… ## $ Species &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, … In general, str (for structure) is a very powerful function to explore the content of a data structure (see next part). To explore it further, you can use the following functions names(iris) ## [1] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; &quot;Species&quot; summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 setosa :50 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 versicolor:50 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 virginica :50 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 plot(iris) # don&#39;t do that with too big data of course ! 3.3.1.5 Functions As mentioned before, R is a functional programming langueage and you can of course create your own functions (which can be afterwards integrated in a package). To cfeate a function, the syntax is such : square &lt;- function(xx=2) # 2 is the default value (not mandatory) { res &lt;- xx^2 return(res) } # Shorthand # square &lt;- function(xx) xx^2 # Use it square() ## [1] 4 square(5) ## [1] 25 3.3.1.6 Exercices Create a vector mixing both numbers and strings : what happens ? Create a vector containing the values “fellow 1” to fellow 15”. Hint : be lazy and use the paste() function Replace the value “fellow 5” with “best fellow” Create a matrix (3,3) of random numbers drawn from a gaussian distribution. Create two numerical matrices of size resp (2,3) and (3,2) filled with 1s and compute their product From the previous list, the second element is a number ; multiply this number by 10 accessing it via its index Create a new list containing the previous list and some other random elements 3.3.2 Explore a new data structure (or object) You will often face new data structures resulting from new functions, and they will be more complicated than the ones we’ve just covered. Let us take the example of the linear regression (which we will cover in section 7) library(ggplot2) ggplot(iris,aes(Petal.Length,Sepal.Length)) + geom_jitter() + geom_smooth(method=&quot;lm&quot;) + theme_minimal() Spoiler alert : the regression aims to find \\(\\alpha\\) and \\(\\beta\\) such that an explained variable \\(y\\) can be expressed as \\(y = \\alpha \\cdot x + \\beta\\) where \\(x\\) is an explanatory variable. In R, to find the values of \\(\\alpha\\) and \\(\\beta\\), you will use the lmfunction. So let’s fit this model and print the result reg &lt;- lm(Petal.Length ~ Sepal.Length,data=iris) reg ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length, data = iris) ## ## Coefficients: ## (Intercept) Sepal.Length ## -7.101 1.858 Ok, that’s really minimal information… Let’s try to dig into this reg object to find more. names(reg) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; ## [8] &quot;df.residual&quot; &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; str(reg) ## List of 12 ## $ coefficients : Named num [1:2] -7.1 1.86 ## ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;Sepal.Length&quot; ## $ residuals : Named num [1:150] -0.9766 -0.6049 -0.3332 0.0527 -0.7907 ... ## ..- attr(*, &quot;names&quot;)= chr [1:150] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ effects : Named num [1:150] -46.026 18.785 -0.207 0.184 -0.679 ... ## ..- attr(*, &quot;names&quot;)= chr [1:150] &quot;(Intercept)&quot; &quot;Sepal.Length&quot; &quot;&quot; &quot;&quot; ... ## $ rank : int 2 ## $ fitted.values: Named num [1:150] 2.38 2 1.63 1.45 2.19 ... ## ..- attr(*, &quot;names&quot;)= chr [1:150] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ assign : int [1:2] 0 1 ## $ qr :List of 5 ## ..$ qr : num [1:150, 1:2] -12.2474 0.0816 0.0816 0.0816 0.0816 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:150] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:2] &quot;(Intercept)&quot; &quot;Sepal.Length&quot; ## .. ..- attr(*, &quot;assign&quot;)= int [1:2] 0 1 ## ..$ qraux: num [1:2] 1.08 1.09 ## ..$ pivot: int [1:2] 1 2 ## ..$ tol : num 1e-07 ## ..$ rank : int 2 ## ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; ## $ df.residual : int 148 ## $ xlevels : Named list() ## $ call : language lm(formula = Petal.Length ~ Sepal.Length, data = iris) ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language Petal.Length ~ Sepal.Length ## .. ..- attr(*, &quot;variables&quot;)= language list(Petal.Length, Sepal.Length) ## .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:2] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; ## .. .. .. ..$ : chr &quot;Sepal.Length&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;Sepal.Length&quot; ## .. ..- attr(*, &quot;order&quot;)= int 1 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(Petal.Length, Sepal.Length) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; ## $ model :&#39;data.frame&#39;: 150 obs. of 2 variables: ## ..$ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## ..$ Sepal.Length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language Petal.Length ~ Sepal.Length ## .. .. ..- attr(*, &quot;variables&quot;)= language list(Petal.Length, Sepal.Length) ## .. .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1 ## .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. .. ..$ : chr [1:2] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; ## .. .. .. .. ..$ : chr &quot;Sepal.Length&quot; ## .. .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;Sepal.Length&quot; ## .. .. ..- attr(*, &quot;order&quot;)= int 1 ## .. .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. .. ..- attr(*, &quot;response&quot;)= int 1 ## .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; ## .. .. ..- attr(*, &quot;predvars&quot;)= language list(Petal.Length, Sepal.Length) ## .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot; ## .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Petal.Length&quot; &quot;Sepal.Length&quot; ## - attr(*, &quot;class&quot;)= chr &quot;lm&quot; That’s more interesting ! It seems that I can get more, including raw data, residuals, coefficients, degrees of freedom… And in general, you can apply standard functions on it as well summary(reg) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.47747 -0.59072 -0.00668 0.60484 2.49512 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.10144 0.50666 -14.02 &lt;2e-16 *** ## Sepal.Length 1.85843 0.08586 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8678 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 plot(reg) Ok, that’s it, I have almost all that I wanted ! We’ll cover the rest at the end of the week ! 3.3.2.1 Exercise On the iris dataset, use the kmeans function to cluster the flowers with respect to Sepal.Length and Petal.Length and try to find your way in the resulting object. If you want to know more about the kmeans algorithm, you can check this video "],["manip.html", "Chapter 4 Data manipulation 4.1 Import 4.2 The grammar of data manipulation 4.3 Let’s import and wrangle some data ! 4.4 Tidy your data", " Chapter 4 Data manipulation In order to manipulate and wrangle data, there are (at least) 3 frameworks available : Base R (not covered) : similar to Pandas Tidyverse/dplyr : high level interface data.table : less friendly user interface but amazingly optimized We’ll cover the tidyverse approach as it provides a very nice and coherent framework for more than data manipulation. The “tidy” comes an original paper from Hadley Wickham which sets those common sense principles Each variable must have its own column. Each observation must have its own row. Each value must have its own cell. You can check further details and visualization on the R for data science online book Those principles have been largely adopted by the community (powered by Rstudio) and created a full parallel dialect in R for almost all data science tasks : the tidyverse which offers a coherent set of features that are, in addition, often nicely optimized (written in C++) 4.1 Import 4.1.1 Text files You can either use the basic read.table and related functions (eg read.csv) which work pretty fine. If the file is large, you might consider tools from other package such as read_csv and others from the readr package (part of the tidyverse) dat &lt;- read.csv(&quot;Data/Sports/Activities_2022.csv&quot;,header = T) head(dat) ## activityId uuidMsb uuidLsb name activityType ## 1 8117908979 1.389057e+18 -8.816745e+18 Pool Swimming lap_swimming ## 2 8112025942 -8.579595e+18 -6.087296e+18 Zwift - WTRL Team Time Trial - Zone 6 (C) virtual_ride ## 3 8111738850 4.389923e+18 -4.813906e+18 Zwift - .. *3.JOKE?(INC#EA2)&#39;s Meetup - Sleepless City virtual_ride ## 4 8106509382 -3.409299e+18 -6.907548e+18 Vienna Running running ## 5 8101019939 8.398595e+18 -7.204024e+18 Zwift - London virtual_ride ## 6 8100932442 -6.116764e+18 -8.563845e+18 Zwift - TdZ Stage 1: Long Ride virtual_ride ## userProfileId timeZoneId beginTimestamp eventTypeId rule sportType startTimeGmt startTimeLocal duration distance ## 1 1141258 124 1.642184e+12 9 public SWIMMING 1.642184e+12 1.642188e+12 4271050 340000 ## 2 1141258 124 1.642096e+12 9 public CYCLING 1.642096e+12 1.642099e+12 2275000 2771270 ## 3 1141258 124 1.642094e+12 9 public CYCLING 1.642094e+12 1.642097e+12 1707000 1640097 ## 4 1141258 124 1.642009e+12 9 public RUNNING 1.642009e+12 1.642012e+12 4224043 1499398 ## 5 1141258 124 1.641928e+12 9 public CYCLING 1.641928e+12 1.641932e+12 622000 624256 ## 6 1141258 124 1.641924e+12 9 public CYCLING 1.641924e+12 1.641928e+12 3954000 4927470 ## avgSpeed avgHr maxHr avgPower avgBikeCadence maxBikeCadence calories aerobicTrainingEffect strokes normPower ## 1 0.0993 NA NA NA NA NA 3033.5745 NA 1390 NA ## 2 1.2181 168 181 333 86 104 3104.8048 NA 0 348.5247 ## 3 0.9608 135 154 240 83 200 1638.2978 NA 0 249.8839 ## 4 0.3550 148 162 NA NA NA 3720.7378 2.4 NA NA ## 5 1.0036 138 151 186 78 100 460.9022 NA 0 194.3214 ## 6 1.2461 130 180 266 84 103 4290.5805 NA 0 275.7212 ## avgLeftBalance avgRightBalance max20MinPower avgFractionalCadence maxFractionalCadence trainingStressScore ## 1 NA NA NA 0.00000 0.0 NA ## 2 NA NA 342.2592 0.00000 0.0 NA ## 3 NA NA 252.1425 0.00000 0.0 NA ## 4 NA NA NA 0.15625 0.5 NA ## 5 NA NA NA 0.00000 0.0 NA ## 6 NA NA 291.5917 0.00000 0.0 NA ## intensityFactor elapsedDuration movingDuration anaerobicTrainingEffect deviceId minTemperature maxTemperature ## 1 NA 4685234 3466198 NA 3968818126 26 27 ## 2 NA 2273000 2270000 NA 3825981698 NA NA ## 3 NA 1706000 1706000 NA 3825981698 NA NA ## 4 NA 4293429 4220701 0 3968818126 7 24 ## 5 NA 620000 618000 NA 3825981698 NA NA ## 6 NA 3953000 3950000 NA 3825981698 NA NA ## lapCount aerobicTrainingEffectMessage anaerobicTrainingEffectMessage purposeful autoCalcCalories favorite pr ## 1 39 NA NA 0 0 0 0 ## 2 1 NA NA 0 0 0 0 ## 3 1 NA NA 0 0 0 0 ## 4 15 NA NA 0 0 0 0 ## 5 1 NA NA 0 0 0 0 ## 6 1 NA NA 0 0 0 0 ## elevationCorrected atpActivity parent elevationGain elevationLoss maxSpeed maxRunCadence steps startLongitude ## 1 0 0 0 NA NA 0.3865 NA NA NA ## 2 0 0 0 32500 0 1.9172 NA NA 0.00000 ## 3 0 0 0 7600 0 1.2852 NA NA 0.00000 ## 4 0 0 0 16000 18300 0.4161 112 12014 16.31589 ## 5 0 0 0 3800 0 1.6083 NA NA 0.00000 ## 6 0 0 0 26600 0 1.7015 NA NA 0.00000 ## startLatitude avgVerticalOscillation avgGroundContactTime avgStrideLength vO2MaxValue avgVerticalRatio ## 1 NA NA NA NA NA NA ## 2 0.00000 NA NA NA NA NA ## 3 0.00000 NA NA NA NA NA ## 4 48.21407 8.26 248.1 118.24 60 6.83 ## 5 0.00000 NA NA NA NA NA ## 6 0.00000 NA NA NA NA NA ## avgGroundContactBalance minElevation maxElevation avgDoubleCadence maxDoubleCadence locationName maxVerticalSpeed ## 1 NA NA NA NA NA &lt;NA&gt; NA ## 2 NA 120 5780 NA NA &lt;NA&gt; 0.11999998 ## 3 NA 12180 13040 NA NA &lt;NA&gt; 0.05999985 ## 4 48.93 19480 34900 172.3125 225 Vienna 0.10000000 ## 5 NA 400 2900 NA NA Tower Hamlets 0.06000004 ## 6 NA 1040 3400 NA NA Thio 0.06000004 ## endLongitude endLatitude avgStrokes activeLengths avgSwolf poolLength avgStrokeDistance avgSwimCadence ## 1 NA NA 22.4 68 73 5000 223 27 ## 2 166.9529124 -11.63601 NA NA NA NA NA NA ## 3 165.8442626 -10.79433 NA NA NA NA NA NA ## 4 16.3033489 48.21414 NA NA NA NA NA NA ## 5 -0.1127895 51.49451 NA NA NA NA NA NA ## 6 166.1724174 -21.73831 NA NA NA NA NA NA ## maxSwimCadence workoutId activeSets totalSets totalReps parentId manufacturer courseId maxFtp avgVerticalSpeed ## 1 31 NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA NA NA ## 4 NA NA NA NA NA NA NA NA NA NA ## 5 NA NA NA NA NA NA NA NA NA NA ## 6 NA NA NA NA NA NA NA NA NA NA ## decoDive lactateThresholdBpm lactateThresholdSpeed ## 1 0 NA NA ## 2 0 NA NA ## 3 0 NA NA ## 4 0 NA NA ## 5 0 NA NA ## 6 0 NA NA You have many options to deal with issues : sep to specify the separator (\\t for tabulation, ';' for semicolon… ) dec the decimal separator encoding the file encoding (special characters from windows/unix systems can be misdetected) colClasses to force one column to be imported in another type than what is detected help(read.table) for more options ! 4.1.2 Excel files You can import excel files (.xls and .xlsx) with the readxl package readxl::read_excel(&quot;Data/Sports/Activities.xlsx&quot;) %&gt;% head() ## # A tibble: 6 × 89 ## activityId uuidMsb uuidLsb name activityType userProfileId timeZoneId beginTimestamp eventTypeId rule sportType ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 5570974040 3.695223… -7.0985… En pi… lap_swimming 1141258 124 1600700451000 9 publ… GENERIC ## 2 5566524321 -5.21279… -8.1406… Vienn… cycling 1141258 124 1600601823000 9 publ… CYCLING ## 3 5561266034 1.271355… -5.5228… Korne… cycling 1141258 124 1600522721000 9 publ… CYCLING ## 4 5555881653 -2.92393… -4.6768… Vienn… running 1141258 124 1600439317000 9 publ… RUNNING ## 5 5551811953 7.859042… -7.9095… Zwift… virtual_ride 1141258 124 1600361706000 9 publ… GENERIC ## 6 5551052200 -9.55408… -6.8664… En pi… lap_swimming 1141258 124 1600353189000 9 publ… GENERIC ## # … with 78 more variables: startTimeGmt &lt;dbl&gt;, startTimeLocal &lt;chr&gt;, duration &lt;chr&gt;, distance &lt;chr&gt;, ## # elevationGain &lt;chr&gt;, elevationLoss &lt;chr&gt;, avgSpeed &lt;chr&gt;, maxSpeed &lt;chr&gt;, avgHr &lt;chr&gt;, maxHr &lt;chr&gt;, ## # calories &lt;chr&gt;, startLongitude &lt;chr&gt;, startLatitude &lt;chr&gt;, aerobicTrainingEffect &lt;chr&gt;, ## # avgFractionalCadence &lt;chr&gt;, maxFractionalCadence &lt;chr&gt;, elapsedDuration &lt;chr&gt;, movingDuration &lt;chr&gt;, ## # anaerobicTrainingEffect &lt;chr&gt;, deviceId &lt;dbl&gt;, minTemperature &lt;chr&gt;, maxTemperature &lt;chr&gt;, minElevation &lt;chr&gt;, ## # maxElevation &lt;chr&gt;, locationName &lt;chr&gt;, maxVerticalSpeed &lt;chr&gt;, lapCount &lt;dbl&gt;, endLongitude &lt;chr&gt;, ## # endLatitude &lt;chr&gt;, activeSets &lt;chr&gt;, totalSets &lt;chr&gt;, totalReps &lt;chr&gt;, purposeful &lt;chr&gt;, autoCalcCalories &lt;chr&gt;, … Options : sheet to select whioch you want to import range : the “zone” of the sheet you want to import (beginning and ending row/column to be provided) col_types to specify the types of the column if misdetected ?readxl::read_excel for more information 4.1.3 More formats The readr package provides other convenient functions to read the most common (open) formats. With haven, you can also read data from proprietary formats (SPSS, SAS, Stat,…). JSON files can be read with for example rjsonlite and we will use it in an applicaiton example. XML and HTML files can be parsed with the xml2package. 4.1.4 Read from databases / big data This is a huge topic that we will only mention here, but for (almost) each database engine, there is a package available in order to be able to read data from databases General purpose : odbc, RODBC, DBI \\(\\rightarrow\\) you will need to install the DB’s drivers Dedicated : RSQLite, RPostgres, RMariaDB (can be used for mySQL too)… \\(\\rightarrow\\) drivers included With the 3 first package, you can connect to “monolith” databases, as well as to distributed databases. You can find more information on the Rstudio website. Another interesting resources is the dbplyr vignette, that describes how to connect to a database and query it using dplyr’s verbs. In addition, the sparlyr package allows you to interact with a spark cluster (using dplyr syntax) 4.2 The grammar of data manipulation Alert : After this section, pandas will appear much less appealing…. Following the tidy data principles, dplyr implements an actual grammar of data manipulation with verbs and human-readable syntax. 4.2.1 The pipe The first operator to know is the pipe operator, %&gt;% which allows you to redirect the output of a command “to the right” and hence create readable chains of commands. Let’s extract the last 3 characters of “hello world” First solution : create useless objects char &lt;- &quot;hello world&quot; rev_char &lt;- stringi::stri_reverse(char) sub3 &lt;- substr(rev_char,1,3) stringi::stri_reverse(sub3) ## [1] &quot;rld&quot; Second solution : where’s the beginning ???? stringi::stri_reverse(substr(stringi::stri_reverse(&quot;Hello World&quot;),1,3)) ## [1] &quot;rld&quot; Third solution : using the pipe &quot;Hello world&quot; %&gt;% stringi::stri_reverse() %&gt;% substr(1,3) %&gt;% stringi::stri_reverse() ## [1] &quot;rld&quot; Under the hood : the dot represents the result of the previous step and can be placed somewhere else in the next function (rather than the first argument) &quot;Hello world&quot; %&gt;% stringi::stri_reverse(.) %&gt;% substr(.,1,3) %&gt;% stringi::stri_reverse(.) ## [1] &quot;rld&quot; 4.2.2 The verbs of manipulation What do you do with data ? Select columns \\(\\rightarrow\\) select() Filter rows \\(\\rightarrow\\) filter() Create / modify columns \\(\\rightarrow\\) mutate() Compute summaries of the columns \\(\\rightarrow\\) summarise() Do group-wise operations \\(\\rightarrow\\) group_by() Join with other tables \\(\\rightarrow\\) left_join(),right_join(), inner_join(), anti_join(), full_join() I have the verbs, now I can associate them to make a sentence ! All those functions take as first argument a dataframe, which makes it very easy when chaining them with the pipe. read.csv(&quot;Data/Sports/Activities.csv&quot;) %&gt;% select(activityType,avgSpeed,distance,startLongitude,startLatitude,sportType) %&gt;% #select some metrics mutate(distance=distance/100) %&gt;% # distances are in decameter (?) filter(activityType!=&quot;other&quot;) %&gt;% # remove activities &quot;other group_by(activityType) %&gt;% summarise(total_dist=mean(distance)) ## # A tibble: 22 × 2 ## activityType total_dist ## &lt;chr&gt; &lt;dbl&gt; ## 1 cross_country_skiing_ws 16717. ## 2 cycling 34124. ## 3 cyclocross 41210. ## 4 hiking 10678. ## 5 indoor_cardio NA ## 6 indoor_cycling NA ## 7 indoor_running 2311. ## 8 lap_swimming 3014. ## 9 multi_sport 63873. ## 10 open_water_swimming 2359. ## # … with 12 more rows 4.2.3 Filter : conditions This is the way you write conditions in R : Syntax Condition == Equality test != Different than %in% c(…) Is in this list of values \\(&gt;, &gt;=\\) \\(&lt;, &lt;=\\) Greater/less than ! (x %in% c(…)) Not in the list &amp; And | Or 4.2.4 Mutate Most of the data manipulation will be done in a mutate statement. This is where you can create additional columns, modify the ones existing. You can do any kind of transformation you want with this one. Depending on the type of the data, here are some additional packages that will help you : lubridate to easily handle date variables forcats to handle factors (categorical variables) stringr (and stringi) to handle strings variables and work with regular expressions ifelse() and case_when() to handle conditional operations require(lubridate) require(stringr) dat &lt;- dat %&gt;% mutate(start_time=as_datetime(startTimeLocal/1000), # create a timestamp date = floor_date(start_time,&quot;day&quot;), # round to the day is_bike=ifelse(activityType %in% c(&quot;cycling&quot;,&quot;virtual_ride&quot;,&quot;indoor_cycling&quot;,&quot;road_biking&quot;,&quot;cyclocross&quot;),T,F), # is it bike or not ? is_run = str_detect(activityType,&quot;running|hicking&quot;), activity_recoded = case_when(is_bike ~ &quot;Bike&quot;, is_run ~ &quot;Run&quot;, str_detect(activityType,&quot;swim&quot;) ~&quot;Swim&quot;, TRUE ~ &quot;Other&quot;)) 4.2.5 Summarize This operation consists in summarizing several rows ofinto one or more synthetic value(s). We will cover the topic more in detail 5 but the most common summary function that you can use are : For continuous variables : average, sum, median, standard deviation, interquartile range (IQR), concentration indexes,… For categorical variables : count, count distinct, concentration indexes,… Simple summary statistics over one numerical variable : dat %&gt;% summarise(total_distance=sum(distance)) # Oups ## total_distance ## 1 NA dat %&gt;% summarise(total_distance=sum(distance,na.rm = T)) ## total_distance ## 1 12460037061 dat %&gt;% summarise(avg_distance=mean(distance,na.rm = T)) ## avg_distance ## 1 2015861 dat %&gt;% summarise(median_distance=median(distance,na.rm = T)) ## median_distance ## 1 1205483 4.2.6 Join with other tables Thanks to the xxx_join(table_a,table_b) functions you can combine several dataframes into one, matching the rows on chosen columns. left_join() will keep the rows coming from table_a right_join() will keep the rows coming from table_b inner_join() will keep the rows present in both tables full_join() will keep the rows from both tables anti_join() will keep the rows from table_a that are not in table_b In the following code, we first compute a summary of distance and power for each combination of activity_recoded and deviceId and combine it together with the initial dataframe in order to compute a “distance percentage” for each activity, as the activity distance divided by the total distance recorded for the same type of activity and with the same device. agg &lt;- dat %&gt;% group_by(activity_recoded,deviceId) %&gt;% summarise(total_distance=sum(distance,na.rm = T), avgPower = mean(avgPower,na.rm=T)) dat %&gt;% select(distance,activityId,activity_recoded,deviceId) %&gt;% left_join(agg,by=c(&quot;activity_recoded&quot;,&quot;deviceId&quot;)) %&gt;% mutate(distance_pct = 100*distance/total_distance) %&gt;% head() ## distance activityId activity_recoded deviceId total_distance avgPower distance_pct ## 1 340000 8117908979 Swim 3968818126 75093816 NaN 0.45276698 ## 2 2771270 8112025942 Bike 3825981698 1237962879 251.009 0.22385727 ## 3 1640097 8111738850 Bike 3825981698 1237962879 251.009 0.13248354 ## 4 1499398 8106509382 Run 3968818126 694501042 NaN 0.21589572 ## 5 624256 8101019939 Bike 3825981698 1237962879 251.009 0.05042607 ## 6 4927470 8100932442 Bike 3825981698 1237962879 251.009 0.39803051 4.2.7 Manipulate several data in the same time With all previous verbs above, you can use the across function to apply the same operation over a bunch of columns that you can select depending a simple enumeration or a condition (on their type or their name). This is a really powerful tool ! Example : we will convert all columns that are identifiers as character variables because the numbers have no meaning dat &lt;- dat %&gt;% mutate(across(c(contains(&quot;Id&quot;),contains(&quot;uuid&quot;)), as.character)) # Other stupid examples dat %&gt;% summarise(across(where(is.numeric), function(xx) sum(xx,na.rm=T))) ## beginTimestamp startTimeGmt startTimeLocal duration distance avgSpeed avgHr maxHr avgPower avgBikeCadence ## 1 9.115198e+15 9.130494e+15 9.130529e+15 25615640953 12460037061 2604.539 622949 736233 244072 148991 ## maxBikeCadence calories aerobicTrainingEffect strokes normPower avgLeftBalance avgRightBalance max20MinPower ## 1 196512 23068547 8194.2 12586771 263186.2 32092.28 32207.72 266312.1 ## avgFractionalCadence maxFractionalCadence trainingStressScore intensityFactor elapsedDuration movingDuration ## 1 498.5859 232 153951.3 677.757 14915022075 9035529534 ## anaerobicTrainingEffect minTemperature maxTemperature lapCount purposeful autoCalcCalories favorite pr ## 1 328.2 54208 75796 35699 0 19 1 8 ## elevationCorrected atpActivity parent elevationGain elevationLoss maxSpeed maxRunCadence steps startLongitude ## 1 6 0 52 181365816 157535613 114630.2 104583 30070036 22364.53 ## startLatitude avgVerticalOscillation avgGroundContactTime vO2MaxValue avgVerticalRatio avgGroundContactBalance ## 1 203830.2 6380.52 184995.2 62584 5628.33 36199.7 ## minElevation maxElevation avgDoubleCadence maxDoubleCadence maxVerticalSpeed endLongitude endLatitude avgStrokes ## 1 44510572 84601757 160554.9 208808 412.4803 24056.88 51257.07 37535.92 ## activeLengths avgSwolf poolLength avgStrokeDistance avgSwimCadence maxSwimCadence activeSets totalSets totalReps ## 1 86201 42980 2300698 128108 23258 30877 0 0 0 ## maxFtp avgVerticalSpeed decoDive lactateThresholdBpm lactateThresholdSpeed ## 1 42787 0 0 12296 28.8716 4.3 Let’s import and wrangle some data ! 4.3.1 The data We will work on the summary data of all past activities, which come in JSON files. So basically, the data is contained in (nested) lists. This is real word data, it’s super messy and dirty ! You will have to : Import the data Import one of the files using jsonlite Inspect and understand the structure of the list Get all the metrics that are included Figure out how to extract one specific metric for one activity Design a function to extract one metric for all activities contained in the JSON Design a function that will extract all metrics for all activities in the JSON Have a first cleaning of the data : Check the distance/elevation variables ; what do you think ? Check the speed related variables : what do you think ? Check the calories variable and adjust it Check the duration related variable and adjust them to have minutes To help you figuring out, you can check an activity on garmin’s site using this url and change the activity number for the one you are inspecting A lot of reverse engineering ahead 4.3.2 One tool you will need : lapply() JSON are lists, and to iterate over list elements, you can either use for loops, which is highly not recommended (R is no good with loops), or use lapply(). This function applies an operation over all elements of a list (or vector) and returns a list containing the result. You can also use sapply() which tries to coerce the result to a vector (if possible) if the expected output is not a list. random_list &lt;- lapply(1:100,function(xx) rnorm(100,xx,xx/5)) str(random_list[1:5]) ## List of 5 ## $ : num [1:100] 0.876 1.185 0.889 1.089 0.784 ... ## $ : num [1:100] 2.861 0.965 2.361 2.17 1.598 ... ## $ : num [1:100] 2.83 3.18 3.26 2.83 2.91 ... ## $ : num [1:100] 4.98 4.11 4.47 3.99 3.22 ... ## $ : num [1:100] 4.6 5.38 4.55 4.9 4.17 ... Attention : be careful with the squared brackets random_list[1] # is a list ## [[1]] ## [1] 0.8762579 1.1847322 0.8894186 1.0885496 0.7842904 1.0868630 0.9707225 0.7042148 0.8791926 1.1901618 1.2059422 ## [12] 0.8361683 0.8028438 0.8453620 1.1883960 0.8592794 1.2346351 0.5714095 0.5868384 0.8403402 0.9547395 0.9232981 ## [23] 0.8736897 1.0154564 1.1842713 0.8953028 0.7105368 0.9654275 1.0736669 1.0171087 1.1385832 0.9779842 1.2855125 ## [34] 0.8600799 1.0656875 0.9532695 1.0905608 1.2626257 0.9034522 1.4887389 1.2437893 1.0017677 1.0482679 1.3534238 ## [45] 1.2382150 1.0077574 1.0306095 0.9037012 1.1903217 0.9172878 1.2586481 1.4085976 0.7471268 1.2147218 1.1365606 ## [56] 0.6883287 1.0184002 1.0261762 1.5795826 0.8519094 0.9071928 1.0488016 1.1892730 1.0604149 0.8092711 1.2777698 ## [67] 1.3484682 1.1167283 0.7210837 1.1141198 1.2526091 0.8732426 0.9369893 1.0553987 1.2096500 1.0153447 1.0896273 ## [78] 1.0623083 0.9412501 1.1729047 0.9554398 0.7967397 1.2279968 0.9001226 1.1318012 0.7118117 1.3900315 1.2528391 ## [89] 0.7792249 0.4889133 1.2154665 1.0288691 0.6976595 0.5154510 0.9041581 1.1290166 0.8295818 0.7789798 0.7826941 ## [100] 1.0047529 random_list[[1]] # is a vector ## [1] 0.8762579 1.1847322 0.8894186 1.0885496 0.7842904 1.0868630 0.9707225 0.7042148 0.8791926 1.1901618 1.2059422 ## [12] 0.8361683 0.8028438 0.8453620 1.1883960 0.8592794 1.2346351 0.5714095 0.5868384 0.8403402 0.9547395 0.9232981 ## [23] 0.8736897 1.0154564 1.1842713 0.8953028 0.7105368 0.9654275 1.0736669 1.0171087 1.1385832 0.9779842 1.2855125 ## [34] 0.8600799 1.0656875 0.9532695 1.0905608 1.2626257 0.9034522 1.4887389 1.2437893 1.0017677 1.0482679 1.3534238 ## [45] 1.2382150 1.0077574 1.0306095 0.9037012 1.1903217 0.9172878 1.2586481 1.4085976 0.7471268 1.2147218 1.1365606 ## [56] 0.6883287 1.0184002 1.0261762 1.5795826 0.8519094 0.9071928 1.0488016 1.1892730 1.0604149 0.8092711 1.2777698 ## [67] 1.3484682 1.1167283 0.7210837 1.1141198 1.2526091 0.8732426 0.9369893 1.0553987 1.2096500 1.0153447 1.0896273 ## [78] 1.0623083 0.9412501 1.1729047 0.9554398 0.7967397 1.2279968 0.9001226 1.1318012 0.7118117 1.3900315 1.2528391 ## [89] 0.7792249 0.4889133 1.2154665 1.0288691 0.6976595 0.5154510 0.9041581 1.1290166 0.8295818 0.7789798 0.7826941 ## [100] 1.0047529 4.4 Tidy your data The data will almost never come in a ready-to-use format. Wrangling the data, beyond cleaning it also sometimes imply to reshape it so that it conforms to the tidy principles. For that you have 2 functions : pivot_longer() which will convert columns into rows pivot_wider(), the reciprocate operation, which will convert rows into columns For example, we can chose that an observation is the combination of an activity and a metric. This representation can be useful in some cases (see 6). dat_long &lt;- dat %&gt;% select(activityId,where(is.numeric)) %&gt;% pivot_longer(-activityId,names_to=&quot;metric&quot;,values_to=&quot;value&quot;) dat_long ## # A tibble: 426,972 × 3 ## activityId metric value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 8117908979 beginTimestamp 1.64e+12 ## 2 8117908979 startTimeGmt 1.64e+12 ## 3 8117908979 startTimeLocal 1.64e+12 ## 4 8117908979 duration 4.27e+ 6 ## 5 8117908979 distance 3.4 e+ 5 ## 6 8117908979 avgSpeed 9.93e- 2 ## 7 8117908979 avgHr NA ## 8 8117908979 maxHr NA ## 9 8117908979 avgPower NA ## 10 8117908979 avgBikeCadence NA ## # … with 426,962 more rows With this format you can get summary statistics for all metrics also easily : group_by(dat_long,metric) %&gt;% summarise(mean_val=mean(value,na.rm=T)) ## # A tibble: 69 × 2 ## metric mean_val ## &lt;chr&gt; &lt;dbl&gt; ## 1 activeLengths 62.1 ## 2 activeSets 0 ## 3 aerobicTrainingEffect 2.64 ## 4 anaerobicTrainingEffect 0.270 ## 5 atpActivity 0 ## 6 autoCalcCalories 0.00621 ## 7 avgBikeCadence 90.1 ## 8 avgDoubleCadence 162. ## 9 avgFractionalCadence 0.0806 ## 10 avgGroundContactBalance 49.3 ## # … with 59 more rows And you you can go back to the original format if you want : group_by(dat_long,metric) %&gt;% summarise(mean_val=mean(value,na.rm=T)) %&gt;% pivot_wider(names_from = metric,values_from=mean_val) ## # A tibble: 1 × 69 ## activeLengths activeSets aerobicTrainingEffect anaerobicTrainingEffect atpActivity autoCalcCalories avgBikeCadence ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 62.1 0 2.64 0.270 0 0.00621 90.1 ## # … with 62 more variables: avgDoubleCadence &lt;dbl&gt;, avgFractionalCadence &lt;dbl&gt;, avgGroundContactBalance &lt;dbl&gt;, ## # avgGroundContactTime &lt;dbl&gt;, avgHr &lt;dbl&gt;, avgLeftBalance &lt;dbl&gt;, avgPower &lt;dbl&gt;, avgRightBalance &lt;dbl&gt;, ## # avgSpeed &lt;dbl&gt;, avgStrokeDistance &lt;dbl&gt;, avgStrokes &lt;dbl&gt;, avgSwimCadence &lt;dbl&gt;, avgSwolf &lt;dbl&gt;, ## # avgVerticalOscillation &lt;dbl&gt;, avgVerticalRatio &lt;dbl&gt;, avgVerticalSpeed &lt;dbl&gt;, beginTimestamp &lt;dbl&gt;, ## # calories &lt;dbl&gt;, decoDive &lt;dbl&gt;, distance &lt;dbl&gt;, duration &lt;dbl&gt;, elapsedDuration &lt;dbl&gt;, elevationCorrected &lt;dbl&gt;, ## # elevationGain &lt;dbl&gt;, elevationLoss &lt;dbl&gt;, endLatitude &lt;dbl&gt;, endLongitude &lt;dbl&gt;, favorite &lt;dbl&gt;, ## # intensityFactor &lt;dbl&gt;, lactateThresholdBpm &lt;dbl&gt;, lactateThresholdSpeed &lt;dbl&gt;, lapCount &lt;dbl&gt;, … The result looks very much like the one we had with across but the intermediate manipulationsncan be very useful in some cases. For instance, you could join the dat_long dataframe or its summary with an external data that has values by metrics (eg the average metric values for pro athletes). "],["stats.html", "Chapter 5 Statistics 5.1 Definitions 5.2 Univariate statistics 5.3 Bivariate statistics 5.4 Statistical inference", " Chapter 5 Statistics Let’s load and clean the data (which you have done during the exercises) source(&quot;ZZ2-data_preparation.R&quot;) This session aims to give a practical guide to explore a dataset you’ve never seen before and to understand some of the key statistical concepts. You will learn to describe each variable of a dataset and assess the strength of the relationship between two variables whatever their types may be. For that, we’ll see how to visually explore a dataset and to quantify what the graphics show. We will also give an overview of what statistical inference is and what it can be used for. This section covers the following topics : Definitions Descriptive statistics Univariate statistics Bivariate statistics Statistical inference : The statistical model Main theorems to be aware of Introduction to statistical tests 5.1 Definitions 5.1.1 Terminology A data set can be viewed in two different manners : A set of rows, or statistical individuals, aka observations (or instances in the galaxy of machine learning). This can be anything A set of columns, or variables that describe the individuals It is crucial to have a good understanding of what the statistical individual is, and that can be challenging ! Some examples : head(dat) ## activityId uuidMsb uuidLsb name ## 1 8117908979 1389057466509903104 -8816744684180752384 Pool Swimming ## 2 8112025942 -8579594978425615360 -6087295519639653376 Zwift - WTRL Team Time Trial - Zone 6 (C) ## 3 8111738850 4389923367750683136 -4813906093171606528 Zwift - .. *3.JOKE?(INC#EA2)&#39;s Meetup - Sleepless City ## 4 8106509382 -3409298562326379008 -6907548221554647040 Vienna Running ## 5 8101019939 8398595148511005696 -7204024095524219904 Zwift - London ## 6 8100932442 -6116764320069431296 -8563845011947501568 Zwift - TdZ Stage 1: Long Ride ## activityType userProfileId timeZoneId beginTimestamp eventTypeId rule sportType startTimeGmt startTimeLocal ## 1 lap_swimming 1141258 124 1.642184e+12 9 public SWIMMING 1.642184e+12 1.642188e+12 ## 2 virtual_ride 1141258 124 1.642096e+12 9 public CYCLING 1.642096e+12 1.642099e+12 ## 3 virtual_ride 1141258 124 1.642094e+12 9 public CYCLING 1.642094e+12 1.642097e+12 ## 4 running 1141258 124 1.642009e+12 9 public RUNNING 1.642009e+12 1.642012e+12 ## 5 virtual_ride 1141258 124 1.641928e+12 9 public CYCLING 1.641928e+12 1.641932e+12 ## 6 virtual_ride 1141258 124 1.641924e+12 9 public CYCLING 1.641924e+12 1.641928e+12 ## duration distance avgSpeed avgHr maxHr avgPower avgBikeCadence maxBikeCadence calories aerobicTrainingEffect ## 1 71.18416 3.40000 3.5748 NA NA NA NA NA 725.0417 NA ## 2 37.91667 27.71270 43.8516 168 181 333 86 104 742.0662 NA ## 3 28.45000 16.40097 34.5888 135 154 240 83 200 391.5626 NA ## 4 70.40072 14.99398 12.7800 148 162 NA NA NA 889.2777 2.4 ## 5 10.36667 6.24256 36.1296 138 151 186 78 100 110.1583 NA ## 6 65.90000 49.27470 44.8596 130 180 266 84 103 1025.4733 NA ## strokes normPower avgLeftBalance avgRightBalance max20MinPower avgFractionalCadence maxFractionalCadence ## 1 1390 NA NA NA NA 0.00000 0.0 ## 2 0 348.5247 NA NA 342.2592 0.00000 0.0 ## 3 0 249.8839 NA NA 252.1425 0.00000 0.0 ## 4 NA NA NA NA NA 0.15625 0.5 ## 5 0 194.3214 NA NA NA 0.00000 0.0 ## 6 0 275.7212 NA NA 291.5917 0.00000 0.0 ## trainingStressScore intensityFactor elapsedDuration movingDuration anaerobicTrainingEffect deviceId minTemperature ## 1 NA NA 78.08723 57.76997 NA 3968818126 26 ## 2 NA NA 37.88333 37.83333 NA 3825981698 NA ## 3 NA NA 28.43333 28.43333 NA 3825981698 NA ## 4 NA NA 71.55715 70.34502 0 3968818126 7 ## 5 NA NA 10.33333 10.30000 NA 3825981698 NA ## 6 NA NA 65.88333 65.83333 NA 3825981698 NA ## maxTemperature lapCount aerobicTrainingEffectMessage anaerobicTrainingEffectMessage purposeful autoCalcCalories ## 1 27 39 NA NA 0 0 ## 2 NA 1 NA NA 0 0 ## 3 NA 1 NA NA 0 0 ## 4 24 15 NA NA 0 0 ## 5 NA 1 NA NA 0 0 ## 6 NA 1 NA NA 0 0 ## favorite pr elevationCorrected atpActivity parent elevationGain elevationLoss maxSpeed maxRunCadence steps ## 1 0 0 0 0 0 NA NA 13.9140 NA NA ## 2 0 0 0 0 0 325 0 69.0192 NA NA ## 3 0 0 0 0 0 76 0 46.2672 NA NA ## 4 0 0 0 0 0 160 183 14.9796 112 12014 ## 5 0 0 0 0 0 38 0 57.8988 NA NA ## 6 0 0 0 0 0 266 0 61.2540 NA NA ## startLongitude startLatitude avgVerticalOscillation avgGroundContactTime avgStrideLength vO2MaxValue ## 1 NA NA NA NA &lt;NA&gt; NA ## 2 0.00000 0.00000 NA NA &lt;NA&gt; NA ## 3 0.00000 0.00000 NA NA &lt;NA&gt; NA ## 4 16.31589 48.21407 8.26 248.1 118.240002441406 60 ## 5 0.00000 0.00000 NA NA &lt;NA&gt; NA ## 6 0.00000 0.00000 NA NA &lt;NA&gt; NA ## avgVerticalRatio avgGroundContactBalance minElevation maxElevation avgDoubleCadence maxDoubleCadence locationName ## 1 NA NA NA NA NA NA &lt;NA&gt; ## 2 NA NA 1.2 57.8 NA NA &lt;NA&gt; ## 3 NA NA 121.8 130.4 NA NA &lt;NA&gt; ## 4 6.83 48.93 194.8 349.0 172.3125 225 Vienna ## 5 NA NA 4.0 29.0 NA NA Tower Hamlets ## 6 NA NA 10.4 34.0 NA NA Thio ## maxVerticalSpeed endLongitude endLatitude avgStrokes activeLengths avgSwolf poolLength avgStrokeDistance ## 1 NA NA NA 22.4 68 73 5000 223 ## 2 4.319999 166.9529124 -11.63601 NA NA NA NA NA ## 3 2.159995 165.8442626 -10.79433 NA NA NA NA NA ## 4 3.600000 16.3033489 48.21414 NA NA NA NA NA ## 5 2.160001 -0.1127895 51.49451 NA NA NA NA NA ## 6 2.160001 166.1724174 -21.73831 NA NA NA NA NA ## avgSwimCadence maxSwimCadence workoutId activeSets totalSets totalReps parentId manufacturer courseId maxFtp ## 1 27 31 &lt;NA&gt; NA NA NA &lt;NA&gt; NA &lt;NA&gt; NA ## 2 NA NA &lt;NA&gt; NA NA NA &lt;NA&gt; NA &lt;NA&gt; NA ## 3 NA NA &lt;NA&gt; NA NA NA &lt;NA&gt; NA &lt;NA&gt; NA ## 4 NA NA &lt;NA&gt; NA NA NA &lt;NA&gt; NA &lt;NA&gt; NA ## 5 NA NA &lt;NA&gt; NA NA NA &lt;NA&gt; NA &lt;NA&gt; NA ## 6 NA NA &lt;NA&gt; NA NA NA &lt;NA&gt; NA &lt;NA&gt; NA ## avgVerticalSpeed decoDive lactateThresholdBpm lactateThresholdSpeed start_time date is_bike is_run ## 1 NA 0 NA NA 2022-01-14 19:14:14 2022-01-14 FALSE FALSE ## 2 NA 0 NA NA 2022-01-13 18:44:12 2022-01-13 TRUE FALSE ## 3 NA 0 NA NA 2022-01-13 18:08:59 2022-01-13 TRUE FALSE ## 4 NA 0 NA NA 2022-01-12 18:30:18 2022-01-12 FALSE TRUE ## 5 NA 0 NA NA 2022-01-11 20:07:09 2022-01-11 TRUE FALSE ## 6 NA 0 NA NA 2022-01-11 19:00:39 2022-01-11 TRUE FALSE ## activity_recoded qual_distance qual_avgHr ## 1 Swim Short &lt;NA&gt; ## 2 Bike Very long &lt;NA&gt; ## 3 Bike Long Average intensity ## 4 Run Long High intensity ## 5 Bike Short Average intensity ## 6 Bike Very long Low intensity group_by(dat,activityType) %&gt;% summarise(total_dist=sum(distance,na.rm=T),avg_speed=mean(avgSpeed,na.rm=T),avg_power=mean(avgPower,na.rm = T), .groups=&quot;keep&quot;) %&gt;% head() ## # A tibble: 6 × 4 ## # Groups: activityType [6] ## activityType total_dist avg_speed avg_power ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 cross_country_skiing_ws 782. 12.6 NaN ## 2 cycling 78241. 23.0 254. ## 3 cyclocross 41.2 18.4 NaN ## 4 hiking 236. 3.85 NaN ## 5 indoor_cardio 0 0 NaN ## 6 indoor_cycling 592. 1.55 225. group_by(dat,date) %&gt;% summarise(total_dist=sum(distance,na.rm=T),avg_speed=mean(avgSpeed,na.rm=T),avg_power=mean(avgPower,na.rm = T), .groups=&quot;keep&quot;) %&gt;% head() ## # A tibble: 6 × 4 ## # Groups: date [6] ## date total_dist avg_speed avg_power ## &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2008-05-27 00:00:00 9.43 21.0 NaN ## 2 2008-11-25 00:00:00 9.25 23.3 NaN ## 3 2008-11-26 00:00:00 19.9 12.7 NaN ## 4 2008-11-27 00:00:00 21.3 22.1 NaN ## 5 2008-11-28 00:00:00 10.6 13.2 NaN ## 6 2008-11-29 00:00:00 0.208 6.90 NaN 5.1.2 Types of variables The way we analyse variables depends on their type : Numerical variables : Continuous : income, revenue \\(\\in \\mathbb{R} , \\mathbb{R}^+\\) Discrete : number of person per household \\(\\in \\mathbb{Z} , \\mathbb{N}\\) Categorical variables : Ordered : small, medium, large Unordered : male, female 5.2 Univariate statistics 5.2.1 Numerical variables 5.2.1.1 Distribution The distribution of a variable quantifies the number of individuals how have a certain value of the variable. We can visualize the distribution either with histograms or density plot, which are the “empirical counterparts” of the probability density function. ggplot(dat,aes(avgPower)) + geom_histogram() + theme_minimal() ggplot(dat,aes(avgPower)) + geom_density() + theme_minimal() 5.2.1.2 Descriptive statistics We typically want to measure what the “average” value is, along with “how diverse is my population.” For that, we can use either sum-based statistics (mean, standard deviation) or quantiles. Quantile-based statistics are said to be robust because much less sensitive to outliers. But they are more computationally expensive. stats &lt;- c(quantile(dat$avgPower,1:3/4,na.rm = T),mean(dat$avgPower,na.rm = T)) ggplot(dat,aes(avgPower)) + geom_histogram() + geom_vline(xintercept = stats,color=&quot;red&quot;) + annotate(geom = &quot;text&quot;,x=stats,y=c(50,40,50,100),label=c(&quot;Q1&quot;,&quot;Q2=median&quot;,&quot;Q3&quot;,&quot;mean&quot;),color=&quot;red&quot;) + geom_segment(aes(x=200,y=25,xend=300,yend=25),color=&quot;blue&quot;) + annotate(geom=&quot;text&quot;,x=300,y=30,label=&quot;dispersion&quot;,color=&quot;blue&quot;) + theme_minimal() 5.2.1.2.1 Central tendency Central tendency statistics allow you to have an idea of the order of magnitude of the attribute you are interested in, over the population. summary(dat$avgPower) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.0 227.0 243.0 245.5 264.0 350.0 5194 quantile(dat$avgPower,probs=0:10/10,na.rm = T) ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 0.0 214.0 224.0 229.0 235.2 243.0 251.0 260.0 269.0 282.7 350.0 5.2.1.2.2 Dispersion Dispersion describes how heterogenous our population is. It can be measured with various measurements (not exhaustive here) sd(dat$avgPower,na.rm = T) # standard deviation ## [1] 31.72316 IQR(dat$avgPower,na.rm = T) # interquartile range ## [1] 37 sd(dat$avgPower,na.rm = T)/mean(dat$avgPower,na.rm = T) # coefficient of variation ## [1] 0.1291948 How to read it : The average deviation to the average power is 29 watts The age difference between the rides in the 25% “less powerful” rides and the 25% “most powerful” rides is 37 watts The average deviation to the average power is 12% of the average power The latter allows to compare dispersion between variables that have different units 5.2.1.3 Dealing with various shapes The traditional example of a distribution is the gaussian distribution fake &lt;- data.frame(xx=rnorm(100000,100,10)) ggplot(fake,aes(xx)) + geom_histogram() + labs(x=&quot;Random variable&quot;)+ theme_minimal() In this case, we have a very interesting property : symmetry, which makes mean and median very close. If the coefficient of variation is not too high, the tail is pretty short. In real life,it (almost) never happens. Therefore, to understand what happens, you can check : How different are mean and median Does a log transformation make the distribution “look better” Is it symmetric \\(\\rightarrow\\) skewness Is flat no not \\(\\rightarrow\\) kurtosis Is the distribution highly concentrated (few individuals get almost the whole cake) \\(\\rightarrow\\) concentration indexes (Gini, enthropy, Herfindahl…). You can check the package ineq Are there outliers (which generates a long tail) \\(\\rightarrow\\) outlier detection (vast field…). You can start with the previous Flat or not flat ? data.frame(xx=rnorm(10000),yy=rnorm(10000,0,5)) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(value,color=name)) + geom_density()+ theme_minimal() ggplot(dat,aes(distance)) + geom_histogram() + theme_minimal() ggplot(dat,aes(distance)) + geom_histogram() + scale_x_log10() + theme_minimal() 5.2.1.4 Exercises : What can you tell about the distance variable ? Draw the distribution of this variable. How much is the maximum distance of the 20% shortest activities ; the minimum distance of the 5% longest activities ? What unit do you think it is ? Did you check the maximum value ? Is there more dispersion in the distance or the average power ? using the facet_wrap function of ggplot2, compare the distributions of distance and avgPower. I want to group activities in 5 categories based on the distance. This operation is called discretization (very useful for choropleth maps). Search for available methods, and apply some of them. Which one is best suited to this variable ? Which one should you avoid ? 5.2.1.5 Bonus : overlay the distribution of two variables If you want to compare the distribution of several variables, you can use several methods with ggplot : Brute-force overlaying 2 geom_density() plots Pivot your data to exhibit an underlying variable that you will use in the aesthetics ggplot(dat) + geom_density(aes(distance),color=&quot;blue&quot;) + geom_density(aes(avgPower)) long_data &lt;- select(dat,distance,avgPower,avgSpeed,activityId) %&gt;% pivot_longer(-activityId,names_to=&quot;metric&quot;,values_to=&quot;val&quot;) ggplot(long_data,aes(val)) + geom_histogram() + facet_wrap(. ~ metric, scales = &quot;free&quot;) ggplot(long_data,aes(x=val,color=metric)) + geom_density() 5.2.2 Categorical variables 5.2.2.1 Working with factors Factors are an optimized way to store categorical variables (encoded in integers). The distinct categories are stored in the level attribute which you can interact with. as.factor(dat$activityType) %&gt;% levels() ## [1] &quot;cross_country_skiing_ws&quot; &quot;cycling&quot; &quot;cyclocross&quot; &quot;hiking&quot; ## [5] &quot;indoor_cardio&quot; &quot;indoor_cycling&quot; &quot;indoor_running&quot; &quot;lap_swimming&quot; ## [9] &quot;multi_sport&quot; &quot;open_water_swimming&quot; &quot;other&quot; &quot;road_biking&quot; ## [13] &quot;running&quot; &quot;street_running&quot; &quot;strength_training&quot; &quot;swimming&quot; ## [17] &quot;swimToBikeTransition_v2&quot; &quot;trail_running&quot; &quot;transition_v2&quot; &quot;treadmill_running&quot; ## [21] &quot;virtual_ride&quot; &quot;walking&quot; as.factor(dat$activityType) %&gt;% str() ## Factor w/ 22 levels &quot;cross_country_skiing_ws&quot;,..: 8 21 21 13 21 21 21 8 13 21 ... For more functionalities you can use the forcats package which provides convenient tools (eg to recode the variable) 5.2.2.2 Barcharts The barchart (which IS NOT a histogram) is the most common representation for categorical variables. You can also use the pie chart (but it requires to hack a little ggplot). Pie charts are despised by the majority of statisticians but it can be adapted if the sizes really differ. Some material to make your own opinion : Why it’s bad Defense ggplot(dat,aes(activityType)) + geom_bar() + coord_flip() + theme_minimal() ggplot(dat,aes(x=&quot;&quot;,fill=activityType)) + geom_bar(width=1) + coord_polar(&quot;y&quot;,start=0) + theme_void() 5.2.2.3 Contingency tables After visualizing, how can we measure the number of cases and the percent in each category ? table(dat$activityType) ## ## cross_country_skiing_ws cycling cyclocross hiking ## 48 2232 1 21 ## indoor_cardio indoor_cycling indoor_running lap_swimming ## 84 330 13 909 ## multi_sport open_water_swimming other road_biking ## 52 74 72 2 ## running street_running strength_training swimming ## 1926 10 4 4 ## swimToBikeTransition_v2 trail_running transition_v2 treadmill_running ## 1 1 63 4 ## virtual_ride walking ## 335 2 table(dat$activityType) %&gt;% prop.table()*100 ## ## cross_country_skiing_ws cycling cyclocross hiking ## 0.77569489 36.06981254 0.01616031 0.33936652 ## indoor_cardio indoor_cycling indoor_running lap_swimming ## 1.35746606 5.33290239 0.21008403 14.68972204 ## multi_sport open_water_swimming other road_biking ## 0.84033613 1.19586296 1.16354234 0.03232062 ## running street_running strength_training swimming ## 31.12475760 0.16160310 0.06464124 0.06464124 ## swimToBikeTransition_v2 trail_running transition_v2 treadmill_running ## 0.01616031 0.01616031 1.01809955 0.06464124 ## virtual_ride walking ## 5.41370394 0.03232062 Another solution is to use what you’ve learned in the previous section (4) : aggregation ! group_by(dat,activityType) %&gt;% summarise(number=n(),proportion=n()/nrow(dat)) ## # A tibble: 22 × 3 ## activityType number proportion ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 cross_country_skiing_ws 48 0.00776 ## 2 cycling 2232 0.361 ## 3 cyclocross 1 0.000162 ## 4 hiking 21 0.00339 ## 5 indoor_cardio 84 0.0136 ## 6 indoor_cycling 330 0.0533 ## 7 indoor_running 13 0.00210 ## 8 lap_swimming 909 0.147 ## 9 multi_sport 52 0.00840 ## 10 open_water_swimming 74 0.0120 ## # … with 12 more rows 5.3 Bivariate statistics In this section, we see how to represent the relationship between two variables and measure it 5.3.1 2 continuous variables 5.3.1.1 Graphical exploration To visualize the relationship between two numerical variables, we can use the scatter plot. Don’t forget that the log function can help you identify non linear relationships since \\(log(a \\cdot x^b) = log(a) + b \\cdot log(x)\\) ggplot(dat,aes(distance,avgPower)) + geom_jitter() + labs(x=&quot;Distance&quot;,y=&quot;Power&quot;,title=&quot;Raw variables&quot;) + scale_x_continuous(labels = scales::comma)+ theme_minimal() ggplot(dat,aes(distance,avgPower)) + geom_jitter() + scale_x_log10(labels = scales::comma) + labs(x=&quot;Distance&quot;,y=&quot;Power&quot;,title=&quot;Distance in log scale&quot;) + theme_minimal() We can pimp up the graphics a bit to visualize the correlation ggplot(dat,aes(distance/1E5,avgPower)) + geom_jitter() + scale_x_log10(labels = scales::comma) + labs(x=&quot;Distance&quot;,y=&quot;Power&quot;,title=&quot;Usage in log scale&quot;)+ geom_smooth(method=&quot;lm&quot;) + theme_minimal() We can see here that there is a positive relationship between distance and data usage and that this relationship has an exponential shape, meaning that the usage increases A LOT when the age drops. 5.3.1.2 Quantifying the relationship : correlations To quantify this relationship, you can use the coefficients of correlation. There are 3 main coefficients : Pearson (the most famous and used), Kendall and Spearman. The latter can handle non-linear functional dependencies (ranks correlation) ; this is (roughly) equivalent to computing the coefficients on the log-transformed variables. # Pearson coeff cor(dat$distance,dat$avgPower,method=&quot;pearson&quot;) ## [1] NA # Pearson coeff, NAs removed cor(dat$distance,dat$avgPower,method=&quot;pearson&quot;,use = &quot;complete.obs&quot;) ## [1] 0.409875 # Spearman coeff cor(dat$distance,dat$avgPower,method=&quot;spearman&quot;,use = &quot;complete.obs&quot;) ## [1] 0.568165 # Why is R a beautiful language ? Do it at once, without loops (loops are evil) print(&quot;all coeffs&quot;) ## [1] &quot;all coeffs&quot; sapply(c(&quot;pearson&quot;,&quot;spearman&quot;,&quot;kendall&quot;),function(xx) cor(dat$distance,dat$avgPower,method=xx,use = &quot;complete.obs&quot;)) ## pearson spearman kendall ## 0.4098750 0.5681650 0.3843185 More info about correlation coefficients Should there be a complex relationship (eg sine), the graphical exploration is mandatory ! 5.3.2 2 categorical variables For this part, I will create a discrete variable out of the distance variable (see previous exercises) to use it as second qualitative variable (the other ones are not really meaningful) dat &lt;- mutate(dat,qual_distance=as.character(cut(distance, quantile(distance,probs = 0:5/5,na.rm=T), include.lowest = T, labels=c(&quot;Very short&quot;,&quot;Short&quot;, &quot;Average&quot;,&quot;Long&quot;,&quot;Very long&quot;))), qual_avgHr=as.character(cut(avgHr,quantile(avgHr,0:3/5,na.rm = T), include.lowest = T, labels=c(&quot;Low intensity&quot;,&quot;Average intensity&quot;, &quot;High intensity&quot;))), qual_distance=ifelse(is.na(qual_distance),&quot;Very short&quot;,qual_distance)) Try different layouts with you barcharts ! 5.3.2.1 Barcharts ggplot(dat,aes(activity_recoded,fill=qual_distance)) + geom_bar(position = &quot;stack&quot;) + theme_minimal() ggplot(dat,aes(activity_recoded,fill=qual_distance)) + geom_bar(position = &quot;dodge&quot;) + theme_minimal() ggplot(dat,aes(activity_recoded,fill=qual_distance)) + geom_bar(position = &quot;fill&quot;) + scale_y_continuous(labels = scales::percent) + theme_minimal() You get really different insights depending on the representation you chose ! 5.3.2.2 Contingency tables table(dat$activity_recoded,dat$qual_distance) ## ## Average Long Short Very long Very short ## Bike 610 332 226 1172 560 ## Other 45 54 49 38 160 ## Run 581 848 402 24 99 ## Swim 0 2 550 2 434 table(dat$activity_recoded,dat$qual_distance) %&gt;% prop.table()*100 ## ## Average Long Short Very long Very short ## Bike 9.85778927 5.36522301 3.65223012 18.93988365 9.04977376 ## Other 0.72721396 0.87265676 0.79185520 0.61409179 2.58564964 ## Run 9.38914027 13.70394312 6.49644473 0.38784745 1.59987072 ## Swim 0.00000000 0.03232062 8.88817065 0.03232062 7.01357466 table(dat$activity_recoded,dat$qual_distance) %&gt;% prop.table(margin = 1)*100 ## ## Average Long Short Very long Very short ## Bike 21.0344828 11.4482759 7.7931034 40.4137931 19.3103448 ## Other 13.0057803 15.6069364 14.1618497 10.9826590 46.2427746 ## Run 29.7338792 43.3981576 20.5731832 1.2282497 5.0665302 ## Swim 0.0000000 0.2024291 55.6680162 0.2024291 43.9271255 table(dat$activity_recoded,dat$qual_distance) %&gt;% prop.table(margin = 2)*100 ## ## Average Long Short Very long Very short ## Bike 49.3527508 26.8608414 18.4189079 94.8220065 44.6927374 ## Other 3.6407767 4.3689320 3.9934800 3.0744337 12.7693536 ## Run 47.0064725 68.6084142 32.7628362 1.9417476 7.9010375 ## Swim 0.0000000 0.1618123 44.8247759 0.1618123 34.6368715 5.3.2.3 Quantifying relationships : \\(\\chi^2\\), Cramer’s V The Chi-square (\\(\\chi^2\\)) statistics is used to measure the distance between the actual distribution of cases among categories of both variables and the distribution if the variables were independent. The higher the X-squared, the higher the divergence with independence, meaning that the variables are likely linked (correlation does not apply to categorical variables). The p-value indicates whether this relationship is statistically significant or not. We will see this in more details in the last chapter (Inference). More info and detailed way to compute the value : this website 5.3.2.4 Extreme examples : Let’s assume we want to assess the relationship between distance and activity type. If the variables are independent, the contingency table would look that way : ## Short Very long Long Average ## Swim 10 10 10 10 ## Bike 10 10 10 10 ## Run 10 10 10 10 ## Other 10 10 10 10 In the opposite situation (full dependency), the contingency table would look like that : ## Short Very long Long Average ## Swim 40 0 0 0 ## Bike 0 40 0 0 ## Run 0 0 40 0 ## Other 0 0 0 40 The \\(\\chi^2\\) statistic measures the “distance” between reality and the first case (independence) Note : if some cells of the contingency table have less than 5 cases, the statistic is not reliable (you’ll get a message in this case) The chi-square suffers 2 main drawbacks : its value depends on the number of observations and the total number of categories \\(\\Rightarrow\\) one cannot compare the \\(\\chi^2\\) values for 2 different tables that have different numbers of underlying observations and number of categories. To deal with that, you can use Cramer’s V, which is a (kind of) normalized \\(\\chi^2\\). You can use the function built in the lsr package. Cramer’s V \\(\\in [0,1]\\) and the higher it is, the more intense the link between both variables. # install.packages(&quot;lsr&quot;) # If not installed table(dat$activity_recoded,dat$qual_distance) %&gt;% lsr::cramersV() ## [1] 0.4505235 Let’s check with our 2 extreme examples : lsr::cramersV(ex_dep[,-5]) ## [1] 1 lsr::cramersV(ex_indep) ## [1] 0 In practice, it is very rare to get high values ; a rule of thumb is that a value around 0.2-0.3 is already “decent.” The \\(\\chi^2\\) p-value (if under 0.05) shows that there is a relationship ; Cramer’s V allows to compare between two tables. 5.3.3 1 continuous, 1 categorical variable In this part, we see how to deal with 2 variables that have different types. The goal remains the same : getting insights about the relationship between those 2 variables and quantify the strength of the link. We will try to assess if there is a connection between the distance and the discipline. 5.3.3.1 Boxplots, violin plots Boxplots are a simple, effective and compact representation of a variable’s distribution. It relies on quantiles. Vilin plots allows to see the ful distribution of both variables # Compute bounds of the boxplot # bounds &lt;- group_by(dat,activity_recoded) %&gt;% # summarise(q1=quantile(distance,.25,na.rm = T), # q2=quantile(distance,.50,na.rm = T), # q3=quantile(distance,.75,na.rm = T), # lower_bound=q1-1.5*IQR(distance,na.rm = T), # upper_bound=q3+1.5*IQR(distance,na.rm = T)) %&gt;% # pivot_longer(-activity_recoded) ggplot(dat,aes(activity_recoded,distance/1E5)) + geom_boxplot() + theme_minimal() # geom_point(data = bounds,aes(activity_recoded,value,color=name)) ggplot(dat,aes(activity_recoded,distance/1E5)) + geom_violin()+ theme_minimal() + scale_y_log10(label=scales::comma) It looks like bike activities are longer than the others ! Big surprise ! 5.3.3.2 Quantifying relationship intensity : \\(\\eta^2\\) The graphics indicate that there is a relationship between activity type and distance (if not, boxplots would have the same shape for all groups). We can assess the strength of the connection with the \\(\\eta^2\\) statistics. Using the decomposition of the variance formula \\(SS_{total} = SS_{between} + SS_{within}\\), \\(\\eta^2\\) is defined as \\(\\eta^2 = \\dfrac{SS_{between}}{SS_{total}} \\in [0,1]\\) 5.3.3.3 Extreme examples : If the variables are independent, the boxplots should look like this (almost no difference in the distributions) : If they are fully “correlated,” the activity variable would explain all variance in the data set : In this case, we see that all the variance lies between the subgroups : there is no dispersion within the groups. Note : In practice, the previous situation will of course never happen, and a categorical variable can’t carry by itself a lot of variance (since the number of possible values are de facto limited). This is also the \\(R^2\\) of the 1-factor ANOVA regression of distance explained by activty type, anova &lt;- aov(distance~activity_recoded,data=dat) # Variance decomposition summary(anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## activity_recoded 3 779537 259846 310.8 &lt;2e-16 *** ## Residuals 6177 5163842 836 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 7 observations deleted due to missingness print(&quot;eta squared&quot;) ## [1] &quot;eta squared&quot; lsr::etaSquared(anova) ## eta.sq eta.sq.part ## activity_recoded 0.1311605 0.1311605 # Alternatively lm(distance~activity_recoded,data=dat) %&gt;% summary() ## ## Call: ## lm(formula = distance ~ activity_recoded, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -31.53 -15.28 -0.98 1.96 1136.16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 31.5284 0.5370 58.712 &lt;2e-16 *** ## activity_recodedOther -16.3343 1.6509 -9.894 &lt;2e-16 *** ## activity_recodedRun -18.6880 0.8467 -22.072 &lt;2e-16 *** ## activity_recodedSwim -28.5564 1.0651 -26.810 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28.91 on 6177 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.1312, Adjusted R-squared: 0.1307 ## F-statistic: 310.8 on 3 and 6177 DF, p-value: &lt; 2.2e-16 In this case, 11.2% of the age variance is explained by the difference in activity types ; it is very high. 5.3.4 Exercises Explore the distribution of the average speed. What can you say about it ? Explore the correlation between average speed and average power For all the the categorical variables, get the frequent category (with table AND dplyr/tidyr) Very important : For the next parts, we will remove the extreme observation that is clearly an error dat_clean &lt;- filter(dat,!(activityId %in% c(407226313,2321338)) &amp; year(date)&gt;=2012) 5.4 Statistical inference 5.4.1 The statistical model We want to measure a characteristic in the general population, let’s say the average distance of all potential activites, and let’s denote it by D. The fundamental assumption of the statistical model is that there is an underlying data-genereting process, which means that D is distributed with a certain probability distribution. The goal of the statistician is to find which distribution it is, and estimate its parameters. The big problem is that it is impossible to observe D on the whole population, and any dataset is only a sample of the general population (which does not really exists). The question is then : how can we estimate the parameters of the true distribution ? \\(\\Rightarrow\\) There is a difference between the sample mean and the population mean (noted \\(\\mu\\)). As a matter of fact the sample mean is an estimator of the population mean. The value of an estimator (often noted \\(\\hat{\\theta}\\)) is a random variable (it depends on the sample), meaning this is not a single deterministic value, but has a probability distribution. Therefore it has an expectation and a variance An estimator is said to be biased if \\(\\mathbb{E}(\\hat{\\theta}) \\neq \\mu\\) ; it is said to be efficient if its variance is minimal. One fundamental hypothesis of the model is that all observations are independent and identically distributed (iid). This is typically not the case for time series, but this hypothesis is, in general, reasonable. 5.4.2 Two fundamental theorems Eventough we cannot observe the true parameter(s), and that the sample mean is an estimator (hence a random variable), 2 theorems save the game : 5.4.2.1 The law of large numbers \\[\\bar{D} = \\dfrac{1}{n} \\sum_{i=1}^n D_i \\xrightarrow[n \\to +\\infty]{a.s.} \\mu\\] In other words, when the sample size n is big enough, the sample mean converges to the population mean \\(\\rightarrow\\) We can estimate this parameter with a simple mean without bias. 5.4.2.2 The central limit theorem (CLT) Probably the most important theorem in statistics, valid whatever the true distribution is \\[\\sqrt{n} \\cdot \\bar{D} \\xrightarrow[n \\to +\\infty]{p} \\mathcal{N} (\\mu,\\sigma^2)\\] Meaning that the sample mean converges in probability to a normal distribution with population parameters at “speed” \\(\\sqrt{n}\\). This is equivalent to : \\[ \\bar{D} - \\mu \\xrightarrow[n \\to +\\infty]{p} \\mathcal{N} (0,\\frac{\\sigma^2}{n})\\] Meaning that : I can quantify “how far” my sample mean is from the true value The larger the sample size, the smaller the average deviation to the true value \\(\\rightarrow\\) the variance of my estimator reduces when the sample size increases. Take away : The sample mean is an estimator of the true value of an underlying “true” mean. The estimator’s value depends on the sample I have This estimator (any estimator) has a variance that I could measure if I had several samples to compute several sample means Probality theory gives us tools to estimate the bias and the variance of an estimator Bias-variance trade-off :\\(\\mathbb{E}((D-\\bar{D})^2)=\\mathbb{E}^2(D-\\bar{D}) + \\mathbb{V}(\\bar{D})\\), in other words : \\(MSE_{\\bar{D}} = bias^2 + \\mathbb{V}(\\bar{D})\\) \\(\\rightarrow\\) see you during ML course ;-) Illustration of the biais-variance trade-off. Let’s assume the true average of the usage is 10, what do you prefer over the following scenarios ? Let’s simulate two distributions (let’s say it is the distribution of 2 different estimators) : One with mean 10 and variance 4 \\(\\rightarrow\\) unbiased The second with mean 10.5 and variance 1 \\(\\rightarrow\\) biased but with low variance In the first case, the estimator is unbiased, but with a higher variance than the second : if we go for it, we take the chance to have an estimate (depending on our sample) of eg 15 or 5, which is almost unlikely to happen with the second estimator, although this second is not centered on the true value. It is up to you to decide, but you generally can’t have both an unbiased and very precise estimator… 5.4.3 Statistical tests 5.4.3.1 Introductory example Knowing the theoretical probability distribution of our estimator, we can assess the likelihood of an hypothesis. For the example, let’s make the hypothesis (\\(\\mathcal{H_0}\\)) that the true mean is 10 and standard deviation is \\(2\\sqrt{n}\\). If this hypothesis is true, thanks to the CTL, the distribution of \\(\\bar{D}\\) would be the following : Now, I can compute my sample mean and check its value against the hypothetical distribution : Obviously my actual value doesn’t fit with my hypothesis : the probability of getting such a sample mean under \\(\\mathcal{H_O}\\) is very small \\(\\rightarrow\\) my hypothesis is very unlikely to be valid \\(\\rightarrow\\) the true mean is probably not 10. 5.4.3.2 Student test A test is defined by its null hypothesis \\(\\mathcal{H_0}\\), the contrapositive (alternative) being \\(\\mathcal{H_1}\\). The general procedure is to set \\(\\mathcal{H_0}\\) such that we can build a test statistic of which we can derive the distribution. In general, statisticians chose a null hypothesis such that they can build a statistic for which they know the distribution \\(\\rightarrow\\) they can compute the probability of a specific value to occur. For the example, we’ll focus on the Student test. The test is meant to check whether the true value of the mean is equal to a specific value. Example, I want to test whether the average distance for all activities is 20km/h. Our test is the following : \\(\\mathcal{H_0}\\) : The average distance of activities is 20 \\(\\mathcal{H_1}\\) : The average distance of activities is not 20 The next important parameter of a test if \\(\\alpha\\), the risk level, meaning the probability we are willing to take to be wrong while accepting \\(\\mathcal{H_0}\\). 5% is a value that is often chosen. Thanks to the CLT, we can build the following test statistic : \\[T = \\sqrt{n} \\dfrac{\\bar{D}-20}{\\sigma} \\hookrightarrow \\mathcal{N}(0,1)\\] Problem : we don’t know the value of the true standard deviation. The final test statistic is distributed with a Student distribution : \\[T = \\sqrt{n} \\dfrac{\\bar{D}-20}{\\hat{s}} \\hookrightarrow \\mathcal{St}_{n-1}\\] where \\(\\hat{s}^2 = \\dfrac{1}{n-1}\\sum_{i=1}^n (D-\\bar{D})^2\\) is the unbiased estimator of the variance. 5.4.3.2.1 Implementation in R : t.test(dat$distance,mu=20) ## ## One Sample t-test ## ## data: dat$distance ## t = 0.4021, df = 6180, p-value = 0.6876 ## alternative hypothesis: true mean is not equal to 20 ## 95 percent confidence interval: ## 19.38535 20.93187 ## sample estimates: ## mean of x ## 20.15861 5.4.3.2.2 Interpretation : If you have one thing to remember : the p-value is the probability to be wrong while rejecting \\(\\mathcal{H_0}\\). In our case, this probability is very small, meaning that we should not consider that the average distance of all activities is 20km/h. In short : all you have to know is what the null hypothesis is, and make your decision depending on the p-value. In general, we reject the null is p-value &lt; 5% (the risk level), but you can decide to be more demanding and chose a lower threshold if you don’t want to make a mistake. Note : If you want to dig deeper, you can test whether the mean is strictly greater than a specific value using the parameter alternative of the t-test function. 5.4.3.3 Student test to compare group means You can also use the Student test to check whether the means of two sub-populations are equal or not. In this case, the tests checks whether the difference in means differs significantly from 0. We’ll check in this example if average distance differs between rides and other activities. For that we have to extract on the one hand the bike activities’ distance and the other activites’ distance on the other hand. We can run the test on those 2 vectors : bike &lt;- filter(dat,is_bike) %&gt;% pull(distance) non_bike &lt;- filter(dat,!is_bike) %&gt;% pull(distance) t.test(bike,non_bike) ## ## Welch Two Sample t-test ## ## data: bike and non_bike ## t = 28.255, df = 5116, p-value &lt; 2.2e-16 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 19.92709 22.89845 ## sample estimates: ## mean of x mean of y ## 31.52841 10.11564 In this case, the null hypothesis is “the difference in means is 0” and the p-value is very very small (almost 0) \\(\\rightarrow\\) we can reject the null without second thoughts, which means there is a significant difference between the 2 sub-groups regarding average distance. 5.4.3.4 Back to our \\(\\chi^2\\) Remember the \\(\\chi^2\\) test is used to assess if 2 categorical variables are independent or not. The null hypothesis in this test is “both variable are independant.” To check that, a test statistic, \\(D^2\\) (X-squared in R output) is built, and under \\(\\mathcal{H_0}\\), it is distributed with a \\(\\chi^2\\) probability distribution. We can can then test the validity of the null depending on the test value. Let’s check if there is a relationship between activity and distance in bins : tab &lt;- table(dat$qual_distance,dat$activity_recoded) tab ## ## Bike Other Run Swim ## Average 610 45 581 0 ## Long 332 54 848 2 ## Short 226 49 402 550 ## Very long 1172 38 24 2 ## Very short 560 160 99 434 tab %&gt;% chisq.test() ## ## Pearson&#39;s Chi-squared test ## ## data: . ## X-squared = 3768, df = 12, p-value &lt; 2.2e-16 In this case, the p-value is again much smaller than 5% \\(\\Rightarrow\\) the probability to be wrong by rejecting the null is again very small… It is hence reasonable to reject the null and we can consider that the two variables are independent, meaning there is a connection between the activity type and the distance. 5.4.4 Other estimators : maximum of likelihood We saw that the sample mean is a good estimator for the population mean. If you assume that the underlying probability distribution of the variable of interest is something else than a normal distribution, you can be interested in estimating another parameter than the mean. Let’s say we want to estimate the \\(s\\) parameter of the Zipf’s law. In this case, you can use the maximum likelihood estimator (MLE). The likelihood is the joint probability of observing the sample I got (which is the product of individual probabilities under the iid hypothesis). \\[L_X(s) = \\prod_{i=1}^n f(k;s,N)\\] Since we state that the underlying probability distribution is Zipf’s law, we can express the likelihood as a function of the \\(s\\) parameter. The idea of the MLE is since this sample happened, it was the most likely to happen, hence the value of \\(s\\) is such that it maximizes the likelihood for this particular sample. And finding the maximum value is something for which we have a few algorithms ! For this estimator, we also know some asymptotic properties that allows to build tests, which you can interpret in the same manner. Namely, you have three tests : Wald test Likelihood ratio test LM test (Lagrange multiplier) All those test statistics are distributed as a \\(\\chi^2\\) distribution MLE is also used for machine learning/econometrics, especially when relationships are non linear \\(\\Rightarrow\\) logistic regression. Notes : Sample mean is the MLE for the exponential family (eg gaussian distribution) MLE is a subset of M-estimators 5.4.5 Exercises : interprete a test you don’t know Run a Kolmogorov–Smirnov test on the distance variable, and the logarithm of this variable. What can you conclude ? In time series analysis, it is crucial to check if a time series is stationary or not. Stationarity means that both mean and variance are constant over time (no drift). If it’s the case, it’s easier to model it. To check that, there are several tests called “unit-root” tests (if there is a unit root, the serie is not stationary). In this example, I simulate a time series and run the Phillips-Perron test. The alternative hypothesis is specified in the output ; please interpret the result. library(tseries) # generation of a random walk Xt &lt;- cumsum(rnorm(100)) series &lt;- data.frame(Xt=Xt) %&gt;% mutate(index=row_number()) ggplot(series,aes(index,Xt)) + geom_line() + theme_minimal() + labs(title=&quot;A random walk&quot;) pp.test(Xt) ## ## Phillips-Perron Unit Root Test ## ## data: Xt ## Dickey-Fuller Z(alpha) = -0.42752, Truncation lag parameter = 3, p-value = 0.99 ## alternative hypothesis: stationary "],["multivar.html", "Chapter 6 Multivariate analysis and dimension reduction 6.1 Multivariate analysis 6.2 Multivariate analysis and dimension reduction 6.3 Dimension reduction 6.4 Visualization bonus : dashboards and reports", " Chapter 6 Multivariate analysis and dimension reduction 6.1 Multivariate analysis 6.1.1 Advanced visualization 6.1.1.1 The grammar of graphics The grammar of graphics was introduced in 2005 by Wilkinson and Leland as a general framework for graphical representation of data. It was adapted by Hadley Wickham in the R package ggplot2. It presents a unique foundation for producing almost every quantitative graphic found in scientific journals, newspapers, statistical packages, and data visualization systems. You can check the paper from Wickham When ploting data, one has to define : What are the aesthetics \\(\\rightarrow\\) the dimensions you want to represent What is the geometry you want to use \\(\\rightarrow\\) the kind of plot you want Ploting options (that come with default values) : scales of the axis fonts and colors for text labels (title, axis titles…) In short, what you have to find is the right combination of aesthetics and geometry that best represent the data. To find the recommended combination, don’t forget to use From data to viz 6.1.1.2 Playing with aesthetics You must define at least 1 dimension for the plot, either continuous or categorical for the x axis. Then you can increase the number of dimensions (ie columns of the data frame) you want to represent : x and y for the axis size : optional (integer) dimension that will represent an additional number color/fill : optional (categorical) dimension reprensented by a color. Color is for line/point geoms, fill for bars/heatmap geoms linetype : optional (categorical) : different type of lines (solid, dotted, dashed…). Only for geometries using lines shape : optional (categorical) : variable that will make the shape of the dot vary. Only for point geometries alpha : optional (continuous) : the transparency of the dots (the lower the value, the more transparent the dot) … More about the aesthetics here You can also use the facet_wrap() or facet_grid() functions to add up to 2 more dimensions with categorical variables (see demo) 6.1.1.3 Geometries Once you chose the variables (dimensions) you want to plot, you have to chose the geometry, that highly depends on the type of variable (numerical or character/factor). Don’t forget to reffer to the website from data to viz if you need some inspiration. A short list of most common geometries : geom_bar for barplots geom_histogram for histograms geom_jitter for scatter plots geom_boxplot &amp; geom_violin for compared density plots geom_tile for heatmaps geom_line for time series geom_text or geom_label for text (annotations) geom_hline and geom_vline for horizontal and vertiacal lines …… # From ggplot(dat_clean,aes(x=distance)) + theme_minimal() ggplot(dat_clean,aes(x=distance)) + geom_histogram()+ theme_minimal() ggplot(dat_clean,aes(distance,avgSpeed)) + geom_jitter()+ theme_minimal() ggplot(dat_clean,aes(distance,avgSpeed,color=activity_recoded)) + geom_jitter()+ theme_minimal() # To ggplot(dat_clean,aes(x=avgSpeed,y=calories,size=duration, color=qual_distance,shape=qual_avgHr)) + geom_jitter() + facet_wrap(.~ activity_recoded,scales = &quot;free&quot;)+ theme_minimal() # Or maybe ggplot(dat_clean,aes(x=avgSpeed,y=elevationGain, size=calories,color=duration)) + geom_jitter() + facet_grid(activity_recoded~qual_distance,scales = &quot;free&quot; )+ theme_minimal() 6.1.1.4 Important options With ggplot, one can make publishable graphics that don’t need to be modified in another software. For that, the most useful functions are : scale_xx_yy : functions that allow you to tweak the axis’ scale, the colors used by either color or fill aesthetics (eg : scale_color_manual()) and other options. labs : allows you to proper label title, axis’ titles, legend titles… theme : allows you to tweak general parameters for the plot (font family, font size, margins, background colors…). You have several theme_xx() functions already defined with different default values for those parameters (eg theme_minimal() or theme_void()) guides : allows you to modify the legend entries Whats you can also do is recode the levels of the factor variables to make them more understanble or reorder them if you want them to be displayed in a specific order. See forcats::fct_recode() and forcats::fct_reorder() Hint : when working with strings, you can force a string to be split in 2 rows with \\n Here is an example : dat_clean %&gt;% ggplot(aes(x=avgSpeed,y=elevationGain,size=calories,color=duration,size=distance)) + geom_jitter() + facet_grid(activity_recoded~qual_distance,scales = &quot;free&quot; ) + theme_minimal() + labs(title=&quot;Great insights&quot;,y=&quot;Total evelation&quot;,x=&quot;Average speed&quot;, color=&quot;Duration&quot;,size=&quot;Total distance&quot;) + # scale_color_manual(values = c(&quot;magenta&quot;,&quot;orange&quot;)) + scale_size_continuous(labels=scales::comma) + scale_y_continuous(labels=scales::comma) + scale_x_continuous(labels=scales::comma)+ theme_minimal() 6.1.1.5 Some tricks with ggplot This approach (grammar of graphics) is very coherent but makes it sometimes difficult. For example, how can I represent the distribution of several variables (and not the distribution of one variable according to different sub-groups -meaning that there is a second dimension -) ? 6.1.1.5.1 The cheater way You can brute-force the graph by superposing different geometries. But first, you’ll have to standardize the variables (they don’t have the same scale). Remember the usage of across to apply a function to a selection of variables. mutate(dat_clean,across(where(is.numeric), function(xx) (xx-mean(xx,na.rm=T))/sd(xx,na.rm=T))) %&gt;% ggplot() + geom_density(aes(distance),color=&quot;blue&quot;) + geom_density(aes(duration),color=&quot;red&quot;)+ theme_minimal() This solution can be also used if you want to superpose different geometries (bars and lines, bars and hlines,…), and is in this case legal :D 6.1.1.5.2 Do it with the tidy philosophy You can reformulate this task as : I want to represent the distribution of one unique variable for 2 subgroups : distance and duration. I have to reshape the data first to create such a variable. For that I will use tidyr::pivot_longer() which helps me to transform columns into rows. reshaped &lt;- select(dat_clean,duration,distance) %&gt;% pivot_longer(cols = everything(),names_to=&quot;latent_variable&quot;,values_to=&quot;values&quot;) reshaped ## # A tibble: 11,600 × 2 ## latent_variable values ## &lt;chr&gt; &lt;dbl&gt; ## 1 duration 71.2 ## 2 distance 3.4 ## 3 duration 37.9 ## 4 distance 27.7 ## 5 duration 28.4 ## 6 distance 16.4 ## 7 duration 70.4 ## 8 distance 15.0 ## 9 duration 10.4 ## 10 distance 6.24 ## # … with 11,590 more rows Now I can use the latent_variable variable as a dimension in a classic ggplot statement, with a prior standardization group_by(reshaped,latent_variable) %&gt;% mutate(values=(values-mean(values,na.rm = T))/sd(values,na.rm = T)) %&gt;% ggplot(aes(values,color=latent_variable)) + geom_histogram()+ theme_minimal() Or you can even skip the standardization step thanks to facets : ggplot(reshaped,aes(values)) + geom_histogram() + facet_wrap(.~latent_variable,scales = &quot;free&quot;)+ theme_minimal() 6.1.1.5.3 Combine different plots with ggpubr ggpubr makes your life much easier to make publication-ready graphics. It allows you for example to combine several ggplot graphics in a grid, regardless of any latent dimension that facet_grid would require. For that you have to store the graphics and “replay” them in a defined grid generated by ggarrange() # install.packages(&quot;ggpubr&quot;) require(ggpubr) gg1 &lt;- ggplot(dat_clean,aes(distance)) + geom_histogram() + labs(title = &quot;Distance distribution&quot;)+ theme_minimal() gg2 &lt;- ggplot(dat_clean,aes(x=avgSpeed,y=elevationGain, size=calories,color=duration,size=distance)) + geom_jitter() + facet_grid(activity_recoded~qual_distance,scales = &quot;free&quot; ) + theme_minimal() + labs(title = &quot;Beautiful but useless plot&quot;)+ theme_minimal() ggarrange(gg1,gg2,ncol = 2,widths = c(1,2)) There are a lot of options in this function (common legend, height and width of each plot,…). You can check the full documentation of this package. Note that you can mix tables and graphics with this function. Tables can be rendered as ggplot object with ggpubr::ggtexttable() 6.1.2 Easily explore an entire dataset Now, you know how to produce one graph including several dimensions. To explore a new dataset and identify the correlations between them, you can visualize at a glance all variables in a datasets and their correlations with GGally 6.1.2.1 Scatter plot matrix The scatter plot matrix shows the correlations between all variables and helps you to spot dependencies between them. We can use either the basic plot() function on the dataframe or the GGally package which provides nice extensions to ggplot. Side note : the ggpairs() function does a lot of computation and can take a lot of time ! \\(\\rightarrow\\) if your dataset is large, you should run it only on a sample of the observations with dplyr::sample_n() or dplyr::sample_frac() or only on a selection of columns. select(dat_clean,distance,duration,avgHr,avgSpeed,avgPower, calories,elevationGain,avgBikeCadence,activity_recoded) %&gt;% plot() # install.packages(&quot;GGally&quot;) require(GGally) select(dat_clean,distance,duration,avgSpeed, calories,activity_recoded) %&gt;% GGally::ggpairs() 6.1.2.2 Correlation plots select(dat_clean,distance,duration,avgHr,avgSpeed,avgPower, calories,elevationGain,avgDoubleCadence,activity_recoded) %&gt;% GGally::ggcorr(geom = &quot;circle&quot;) 6.2 Multivariate analysis and dimension reduction In this section, we will focus on the bike activities, which have the highest number of metrics. However, before we can go further, we have to deal with missing data and scale them to avoid that the column with a large order of magnitude are over-weighted. 6.2.1 Imputation So far, we have ignored the missing values because we were computing summary statistics on one or 2 variables. The problem when taking into account more columns is that the probability of having one missing value on one of these features is higher, and therefore the risk that the whole observation is ignored increases. Every observation should still contain some original information that an analysis (or a model) should reflect. To avoid to ignore to many observations because of missing data we perform imputation, meaning that we replace the missing value with a true value. Many methods can be used to that end : For numeric variables Imputation with a random value from the sample Imputation with the mean/median Imputation with the nearest neighbours (k-nn) Hotdeck Imputation with a model (regression) For categorical variables : Imputation with random category selection Imputation with the most frequent category (total or of the neighbours) Hotdeck Model-based imputation The challenge here is to chose between “reflecting the instance’s originality” or “not creating noise” The following code counts the number of missing values per column and does a simple mean or median imputation. We will practice hte imputation via regression in the final chapter’s exercises. dat_bike &lt;- filter(dat_clean,is_bike) # Bike activities sapply(dat_bike,function(xx) sum(is.na(xx))) ## activityId uuidMsb uuidLsb ## 0 934 934 ## name activityType userProfileId ## 230 0 0 ## timeZoneId beginTimestamp eventTypeId ## 0 0 0 ## rule sportType startTimeGmt ## 0 1263 0 ## startTimeLocal duration distance ## 0 0 1 ## avgSpeed avgHr maxHr ## 2 666 665 ## avgPower avgBikeCadence maxBikeCadence ## 1842 1198 1198 ## calories aerobicTrainingEffect strokes ## 2 1521 1207 ## normPower avgLeftBalance avgRightBalance ## 1842 2192 2192 ## max20MinPower avgFractionalCadence maxFractionalCadence ## 1876 0 0 ## trainingStressScore intensityFactor elapsedDuration ## 2177 2177 1191 ## movingDuration anaerobicTrainingEffect deviceId ## 1551 2350 853 ## minTemperature maxTemperature lapCount ## 1288 1288 1644 ## aerobicTrainingEffectMessage anaerobicTrainingEffectMessage purposeful ## 2835 2835 0 ## autoCalcCalories favorite pr ## 1247 0 0 ## elevationCorrected atpActivity parent ## 873 1830 476 ## elevationGain elevationLoss maxSpeed ## 175 176 494 ## maxRunCadence steps startLongitude ## 2834 2834 582 ## startLatitude avgVerticalOscillation avgGroundContactTime ## 582 2835 2835 ## avgStrideLength vO2MaxValue avgVerticalRatio ## 2834 2608 2835 ## avgGroundContactBalance minElevation maxElevation ## 2835 1019 1019 ## avgDoubleCadence maxDoubleCadence locationName ## 2834 2834 1969 ## maxVerticalSpeed endLongitude endLatitude ## 1710 2420 2420 ## avgStrokes activeLengths avgSwolf ## 2835 2653 2835 ## poolLength avgStrokeDistance avgSwimCadence ## 2835 2835 2835 ## maxSwimCadence workoutId activeSets ## 2835 2715 2198 ## totalSets totalReps parentId ## 2198 2198 2822 ## manufacturer courseId maxFtp ## 2835 2832 2737 ## avgVerticalSpeed decoDive lactateThresholdBpm ## 2835 2730 2835 ## lactateThresholdSpeed start_time date ## 2835 0 0 ## is_bike is_run activity_recoded ## 0 0 0 ## qual_distance qual_avgHr ## 0 1051 For this use case, we will use a median imputation, because for some of the variables, the mean would not make sense (eg longitude and latitude). dat_bike_imp &lt;- mutate(dat_bike,across(where(is.numeric), function(xx) ifelse(is.na(xx),median(xx,na.rm=T),xx))) sapply(dat_bike_imp,function(xx) sum(is.na(xx))) ## activityId uuidMsb uuidLsb ## 0 934 934 ## name activityType userProfileId ## 230 0 0 ## timeZoneId beginTimestamp eventTypeId ## 0 0 0 ## rule sportType startTimeGmt ## 0 1263 0 ## startTimeLocal duration distance ## 0 0 0 ## avgSpeed avgHr maxHr ## 0 0 0 ## avgPower avgBikeCadence maxBikeCadence ## 0 0 0 ## calories aerobicTrainingEffect strokes ## 0 0 0 ## normPower avgLeftBalance avgRightBalance ## 0 0 0 ## max20MinPower avgFractionalCadence maxFractionalCadence ## 0 0 0 ## trainingStressScore intensityFactor elapsedDuration ## 0 0 0 ## movingDuration anaerobicTrainingEffect deviceId ## 0 0 853 ## minTemperature maxTemperature lapCount ## 0 0 0 ## aerobicTrainingEffectMessage anaerobicTrainingEffectMessage purposeful ## 2835 2835 0 ## autoCalcCalories favorite pr ## 0 0 0 ## elevationCorrected atpActivity parent ## 0 0 0 ## elevationGain elevationLoss maxSpeed ## 0 0 0 ## maxRunCadence steps startLongitude ## 0 0 0 ## startLatitude avgVerticalOscillation avgGroundContactTime ## 0 2835 2835 ## avgStrideLength vO2MaxValue avgVerticalRatio ## 2834 0 2835 ## avgGroundContactBalance minElevation maxElevation ## 2835 0 0 ## avgDoubleCadence maxDoubleCadence locationName ## 0 0 1969 ## maxVerticalSpeed endLongitude endLatitude ## 0 0 0 ## avgStrokes activeLengths avgSwolf ## 2835 0 2835 ## poolLength avgStrokeDistance avgSwimCadence ## 2835 2835 2835 ## maxSwimCadence workoutId activeSets ## 2835 2715 0 ## totalSets totalReps parentId ## 0 0 2822 ## manufacturer courseId maxFtp ## 2835 2832 0 ## avgVerticalSpeed decoDive lactateThresholdBpm ## 2835 0 2835 ## lactateThresholdSpeed start_time date ## 2835 0 0 ## is_bike is_run activity_recoded ## 0 0 0 ## qual_distance qual_avgHr ## 0 1051 Let’s do the same for categorical (maximum frequency), even though it is not mandatory for PCA. We first have to create a function that will return the most frequent category of a vector. most_freq_cat &lt;- function(xx) { tab &lt;- table(xx) return(names(tab[which.max(tab)])) } dat_bike_imp &lt;- mutate(dat_bike_imp,across(where(is.character), function(xx) coalesce(xx,most_freq_cat(xx)))) sapply(dat_bike_imp,function(xx) sum(is.na(xx))) ## activityId uuidMsb uuidLsb ## 0 0 0 ## name activityType userProfileId ## 0 0 0 ## timeZoneId beginTimestamp eventTypeId ## 0 0 0 ## rule sportType startTimeGmt ## 0 0 0 ## startTimeLocal duration distance ## 0 0 0 ## avgSpeed avgHr maxHr ## 0 0 0 ## avgPower avgBikeCadence maxBikeCadence ## 0 0 0 ## calories aerobicTrainingEffect strokes ## 0 0 0 ## normPower avgLeftBalance avgRightBalance ## 0 0 0 ## max20MinPower avgFractionalCadence maxFractionalCadence ## 0 0 0 ## trainingStressScore intensityFactor elapsedDuration ## 0 0 0 ## movingDuration anaerobicTrainingEffect deviceId ## 0 0 0 ## minTemperature maxTemperature lapCount ## 0 0 0 ## aerobicTrainingEffectMessage anaerobicTrainingEffectMessage purposeful ## 2835 2835 0 ## autoCalcCalories favorite pr ## 0 0 0 ## elevationCorrected atpActivity parent ## 0 0 0 ## elevationGain elevationLoss maxSpeed ## 0 0 0 ## maxRunCadence steps startLongitude ## 0 0 0 ## startLatitude avgVerticalOscillation avgGroundContactTime ## 0 2835 2835 ## avgStrideLength vO2MaxValue avgVerticalRatio ## 0 0 2835 ## avgGroundContactBalance minElevation maxElevation ## 2835 0 0 ## avgDoubleCadence maxDoubleCadence locationName ## 0 0 0 ## maxVerticalSpeed endLongitude endLatitude ## 0 0 0 ## avgStrokes activeLengths avgSwolf ## 2835 0 2835 ## poolLength avgStrokeDistance avgSwimCadence ## 2835 2835 2835 ## maxSwimCadence workoutId activeSets ## 2835 0 0 ## totalSets totalReps parentId ## 0 0 0 ## manufacturer courseId maxFtp ## 2835 0 0 ## avgVerticalSpeed decoDive lactateThresholdBpm ## 2835 0 2835 ## lactateThresholdSpeed start_time date ## 2835 0 0 ## is_bike is_run activity_recoded ## 0 0 0 ## qual_distance qual_avgHr ## 0 0 # replacements &lt;- select(dat_bike_imp,activityId,where(is.character)) %&gt;% # pivot_longer(-activityId,names_to=&quot;name&quot;,values_to=&quot;val&quot;) %&gt;% # filter(!is.na(val)) %&gt;% # group_by(name,val) %&gt;% # summarise(cat_nb=n()) %&gt;% # arrange(name,-cat_nb) %&gt;% # group_by(name) %&gt;% # summarise(most_freq=first(val)) %&gt;% # pivot_wider(names_from = name,values_from=most_freq) %&gt;% # rename_with(function(xx) paste0(xx,&quot;_imp&quot;)) Some of the variables contain only missing values (and cannot be imputed) \\(\\rightarrow\\) we drop them missing &lt;- which(sapply(dat_bike_imp,function(xx) sum(is.na(xx)))&gt;0) %&gt;% names() dat_bike_imp &lt;- select(dat_bike_imp,-missing) 6.2.2 Normalization Normalization is the operation consisting in scaling the columns so that their unit do not matter in the end. For example, the distance in meter is much larger than the cadence or the power, which have totally different units. To normalize the columns and make them unit-less, there are several methods among which the most common are the following : Standardization : \\(X_i^{std} = \\dfrac{X_i-\\bar{X}}{\\sigma_X} \\rightarrow\\) mean 0 and standard deviation 1 Min-Max scaling : \\(X_i^{std} = \\dfrac{X_i-min(X)}{max(X)-min(X)} \\rightarrow\\) between 0 and 1 Robust standardization \\(X_i^{std} = \\dfrac{X_i-Q2(X)}{Q3(X)-Q1(X)} \\rightarrow\\) similar to the first option but robust to outliers You can check scikit-learn’s documentation to see what other options you have (and then search for their R implementation). Normalization is a mandatory step before fitting a model, in order to avoid that only one feature bears the majority of the variance and makes the model biased. 6.2.3 PCA Principal Components Analysis (PCA) is often seen by machine learning engineers “only” as a dimension reduction technique, but it is also a very powerful tool to explore your data. PCA applies on numerical variables only, and aims to create new synthetic and uncorrelated variables : principal components (as a linear combination of the original variables) such that the inertia (ie variance) is highly concentrated on a small number of variables. The mathematical problem is to find eigenvalues and eigenvectors of the correlation matrix. The eigenvectors represent the linear combination of the original variables needed to design those new variables, and the eigenvalues the variance that each of these new value bears. Once those new variables have been found, graphical representations within a few dimensions are possible. To apply PCA, we will use the FactoMineR package, very easy to use for multivariate analysis. Notes : You can have supplementary continuous or categorical variables. They won’t be used in the construction of the eigenvectors, but you will be able to place them in the newly defined vector space. By default, almost all implementations of PCA standardizes the data, so you do not have to do it by yourself, but check in the documentation of the function you use # install.packages(&quot;FactoMineR&quot;) require(FactoMineR) acp_dat &lt;- select(dat_bike_imp,deviceId,duration,distance,elevationGain,elevationLoss,avgSpeed,avgHr,calories, minTemperature,maxTemperature,lapCount,avgBikeCadence,avgPower,vO2MaxValue, max20MinPower) acp &lt;- PCA(acp_dat,graph = F,quali.sup = c(1)) What’s in this new object ? str(acp) ## List of 6 ## $ eig : num [1:14, 1:3] 4.49 2.01 1.67 1.36 1.25 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:14] &quot;comp 1&quot; &quot;comp 2&quot; &quot;comp 3&quot; &quot;comp 4&quot; ... ## .. ..$ : chr [1:3] &quot;eigenvalue&quot; &quot;percentage of variance&quot; &quot;cumulative percentage of variance&quot; ## $ var :List of 4 ## ..$ coord : num [1:14, 1:5] 0.781 0.897 0.503 0.498 0.643 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:14] &quot;duration&quot; &quot;distance&quot; &quot;elevationGain&quot; &quot;elevationLoss&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ cor : num [1:14, 1:5] 0.781 0.897 0.503 0.498 0.643 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:14] &quot;duration&quot; &quot;distance&quot; &quot;elevationGain&quot; &quot;elevationLoss&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ cos2 : num [1:14, 1:5] 0.61 0.805 0.253 0.248 0.413 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:14] &quot;duration&quot; &quot;distance&quot; &quot;elevationGain&quot; &quot;elevationLoss&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ contrib: num [1:14, 1:5] 13.58 17.92 5.64 5.53 9.2 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:14] &quot;duration&quot; &quot;distance&quot; &quot;elevationGain&quot; &quot;elevationLoss&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## $ ind :List of 4 ## ..$ coord : num [1:2835, 1:5] 2.388 -0.877 -1.489 1.153 1.503 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2835] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ cos2 : num [1:2835, 1:5] 0.148 0.157 0.125 0.201 0.267 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2835] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ contrib: num [1:2835, 1:5] 0.04478 0.00604 0.01741 0.01044 0.01774 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:2835] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ dist : Named num [1:2835] 6.22 2.22 4.21 2.57 2.91 ... ## .. ..- attr(*, &quot;names&quot;)= chr [1:2835] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ svd :List of 3 ## ..$ vs: num [1:14] 2.12 1.42 1.29 1.17 1.12 ... ## ..$ U : num [1:2835, 1:5] 1.127 -0.414 -0.703 0.544 0.709 ... ## ..$ V : num [1:14, 1:5] 0.369 0.423 0.238 0.235 0.303 ... ## $ quali.sup:List of 5 ## ..$ coord : num [1:10, 1:5] 3.50332 0.58334 0.47068 -0.12367 0.00898 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:10] &quot;3355158044&quot; &quot;3454054575&quot; &quot;3825981698&quot; &quot;3842681113&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ cos2 : num [1:10, 1:5] 8.00e-01 1.25e-01 1.14e-01 9.20e-03 4.37e-05 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:10] &quot;3355158044&quot; &quot;3454054575&quot; &quot;3825981698&quot; &quot;3842681113&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ v.test: num [1:10, 1:5] 10.65797 0.38925 4.32709 -0.13055 0.00734 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:10] &quot;3355158044&quot; &quot;3454054575&quot; &quot;3825981698&quot; &quot;3842681113&quot; ... ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## ..$ dist : Named num [1:10] 3.92 1.65 1.4 1.29 1.36 ... ## .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;3355158044&quot; &quot;3454054575&quot; &quot;3825981698&quot; &quot;3842681113&quot; ... ## ..$ eta2 : num [1, 1:5] 0.1774 0.0959 0.0819 0.2092 0.0293 ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr &quot;deviceId&quot; ## .. .. ..$ : chr [1:5] &quot;Dim.1&quot; &quot;Dim.2&quot; &quot;Dim.3&quot; &quot;Dim.4&quot; ... ## $ call :List of 10 ## ..$ row.w : num [1:2835] 0.000353 0.000353 0.000353 0.000353 0.000353 ... ## ..$ col.w : num [1:14] 1 1 1 1 1 1 1 1 1 1 ... ## ..$ scale.unit: logi TRUE ## ..$ ncp : num 5 ## ..$ centre : num [1:14] 77.4 31.4 376.1 325.7 22 ... ## ..$ ecart.type: num [1:14] 57.6 34.3 1079.8 1059.3 12.4 ... ## ..$ X :&#39;data.frame&#39;: 2835 obs. of 15 variables: ## .. ..$ deviceId : Factor w/ 10 levels &quot;3355158044&quot;,&quot;3454054575&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## .. ..$ duration : num [1:2835] 37.9 28.4 10.4 65.9 71.5 ... ## .. ..$ distance : num [1:2835] 27.71 16.4 6.24 49.27 52.1 ... ## .. ..$ elevationGain : num [1:2835] 325 76 38 266 266 ... ## .. ..$ elevationLoss : num [1:2835] 0 0 0 0 0 0 0 0 0 0 ... ## .. ..$ avgSpeed : num [1:2835] 43.9 34.6 36.1 44.9 43.7 ... ## .. ..$ avgHr : int [1:2835] 168 135 138 130 158 149 141 149 152 149 ... ## .. ..$ calories : num [1:2835] 742 392 110 1025 1124 ... ## .. ..$ minTemperature: int [1:2835] 18 18 18 18 18 18 18 18 18 18 ... ## .. ..$ maxTemperature: int [1:2835] 27 27 27 27 27 27 27 27 27 27 ... ## .. ..$ lapCount : int [1:2835] 1 1 1 1 1 1 1 1 1 1 ... ## .. ..$ avgBikeCadence: int [1:2835] 86 83 78 84 88 85 74 69 88 86 ... ## .. ..$ avgPower : int [1:2835] 333 240 186 266 270 241 239 280 270 228 ... ## .. ..$ vO2MaxValue : int [1:2835] 65 65 65 65 65 65 65 65 65 65 ... ## .. ..$ max20MinPower : num [1:2835] 342 252 278 292 299 ... ## ..$ row.w.init: num [1:2835] 1 1 1 1 1 1 1 1 1 1 ... ## ..$ call : language PCA(X = acp_dat, quali.sup = c(1), graph = F) ## ..$ quali.sup :List of 5 ## .. ..$ quali.sup :&#39;data.frame&#39;: 2835 obs. of 1 variable: ## .. .. ..$ deviceId: Factor w/ 10 levels &quot;3355158044&quot;,&quot;3454054575&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## .. ..$ modalite : int 10 ## .. ..$ nombre : num [1:10] 41 2 335 5 3 90 15 309 1550 485 ## .. ..$ barycentre:&#39;data.frame&#39;: 10 obs. of 14 variables: ## .. .. ..$ duration : num [1:10] 145.5 96.9 61.9 65.9 110.1 ... ## .. .. ..$ distance : num [1:10] 77.2 42.4 37 34.3 30.5 ... ## .. .. ..$ elevationGain : num [1:10] 700 298 367 250 490 ... ## .. .. ..$ elevationLoss : num [1:10] 706 271 0 261 468 ... ## .. .. ..$ avgSpeed : num [1:10] 31.92 25.22 35.89 28.42 9.52 ... ## .. .. ..$ avgHr : num [1:10] 139 145 140 125 134 ... ## .. .. ..$ calories : num [1:10] 2238 1894 907 593 1018 ... ## .. .. ..$ minTemperature: num [1:10] 15.2 18 18 18 18 ... ## .. .. ..$ maxTemperature: num [1:10] 23.5 27 27 27 27 ... ## .. .. ..$ lapCount : num [1:10] 17.71 3 6.97 3 3 ... ## .. .. ..$ avgBikeCadence: num [1:10] 85.2 95.5 86.4 84.6 85.3 ... ## .. .. ..$ avgPower : num [1:10] 257 243 251 243 243 ... ## .. .. ..$ vO2MaxValue : num [1:10] 68.6 65 65 65 65 ... ## .. .. ..$ max20MinPower : num [1:10] 309 278 281 278 278 ... ## .. ..$ numero : num 1 ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;PCA&quot; &quot;list&quot; This is a list with several elements : eig contains the eigenvalues, and their share in the total inertia (PCA is performed by default on scaled variables \\(\\rightarrow\\) the trace of the diagonal matrix equals to the number of columns) ind and var refer to rows and columns. They both have the same elements : “coord” for the coordinates on each principal component, “contr” the contributions to the inertia of this row (resp column) to the inertia of the component, “cos2” the squared cosine (ie quality of projection) for the row/column quali.sup have the same elements than the previous ones (coordinates are the barycenter of each category of the qualitative variable) plus the \\(\\eta^2\\) statistic. The first thing is to have a look at the eignevalues to see how well the PCA could summarize the information barplot(acp$eig[,2]) barplot(acp$eig[,3]) In our case, the first component bears 35% of the total inertia and the second 15%. Hence, the first factorial plane (the first 2 components) : let’s look at how the variable correlate with them : plot.PCA(acp,choix=&quot;var&quot;,col.var = &quot;blue&quot;) What you can read at once on this plot is how important each variable is to construct the new variables. As a matter of fact, contribution, cos2 and coordinates are almost the same in PCA, meaning that the closer to the unit disc the coordinate is, the higher both contribution and quality of representation. What we deduct from this graph : Distance, calories and duration have the largest values on the first component \\(\\rightarrow\\) they contribute largely to the first component, which bears the 35% of total variance \\(\\rightarrow\\) those variables are the most discriminant Those three variables are very close to each other, which means they are highly correlated The average cadence is on the other side of the first component \\(\\rightarrow\\) it is negatively correlated with those variables There is a right angle between elevationGain and avgPower \\(\\rightarrow\\) the correlation is very small Let’s check the correlation matrix to be sure about what we are reading here select(acp_dat,-deviceId) %&gt;% GGally::ggcorr() How about the individuals ? plot.PCA(acp,choix=&quot;ind&quot;) This graph is harder to read, but we still can identify outliers and understand what they are. Anyway in the top-right corner, those are long rides with a lot of ups and downs ! The most extreme is 742999663 which is another measurement error (indoor cycling with 34000 elevation meters !) 6.2.4 Exercises Remove the measurement errors and re-run the PCA ; what are the changes ? Let’s focus the running activities : select relevant features, impute the missing values using k-nn, run a PCA and analyze the results 6.3 Dimension reduction Dimension reduction aims to reduce the size of the data ; most of the time the goal is to decrease the number of columns but it can also apply to rows (depending on your use case). Why reduce the dimension : Avoid the curse of dimensionality Remove undesired noise Avoid multicollinearity in the features Dimension reduction is, after normalization, a very common step to take prior training a machine learning algorithm. 6.3.1 Use the results of the PCA After performing a PCA, you can select a limited number of principal components that you will use further in the modeling task. You can chose the number of components to keep with several criteria : average inertia (eigenvalue &gt; 1) minimal total inertia (eg 80%) elbow criteria : keep components before there is a “drop” in the eigenvalues barplot 6.3.2 Other inertia-based methods PCA is designed only for continuous variables, but you can use other methods for other types of data : MCA (Multiple Correspondence Analysis ) for categorical variables (if you have a mixture of continuous and categorical, you can discretize numerical variables and perform MCA) FDA (Functional Data Analysis for functions/curves) 6.3.3 t-SNE 6.3.3.1 Algorithm description t-SNE has a completely different approach to dimension reduction : it does not aim to preserve the total variance, but rather the proximity between the observations. The algorithm has 3 steps : Compute the similarity between each pair of points using a gaussian distribution (instead of a raw euclidean distance) \\(\\Rightarrow\\) n collections (distributions) of similarity scores in the high-dimensional space Initiate a low-dimensional space and map all high-dimensional points on it. Compute the same pairwise similarities with Student distribution (hence the t in t-SNE) in this space \\(\\Rightarrow\\) n collections (distributions) of similarity scores in the low-dimensional space Gradually move the points in the low-dimensional space so that the sum of divergences between pairwise similarities in both spaces are minimal. The divergence is the Kullback-Leibler divergence between the 2 distributions of similarity scores You can watch this very well done video to understand how the algorithm precisely works How this similarity scores are computed. dd &lt;- data.frame(xx=rnorm(100),yy=rnorm(100)) %&gt;% mutate(col=ifelse(row_number()==1,&quot;blue&quot;,&quot;black&quot;)) dd %&gt;% ggplot(aes(xx,yy,color=col)) + geom_point() + theme_minimal() + guides(color=F) + scale_color_manual(values = c(&quot;black&quot;,&quot;blue&quot;)) dd %&gt;% mutate(xx_ref=dd$xx[1],yy_ref=dd$yy[1], euclid = (yy_ref-yy)^2 + (yy_ref-yy)^2, simil = 100*dnorm(euclid)) %&gt;% ggplot(aes(euclid,simil)) + geom_point() + theme_minimal() + labs(title=&quot;Distance and similarity to the first point&quot;) Pros and cons compared to PCA : Preserves non-linear relationships and is robust to outliers Much more computationally costly Starts with a PCA 6.3.3.2 Implementation in R # install.packages(&quot;Rtsne&quot;) require(Rtsne) tsne &lt;- select(dat_bike_imp,where(is.numeric)) %&gt;% Rtsne(dims=2,max_iter=500,check_duplicates = F) tsne$Y %&gt;% as.data.frame() %&gt;% ggplot(aes(V1,V2)) + geom_point() + theme_minimal() + labs(title=&quot;Low dimensional representation by t-SNE&quot;) You can check this other extensive blogpost with use cases and also python implementation. 6.3.4 A word about clustering Clustering (or unsupervised machine learning) can also be considered as a dimension reduction technique, but which you operate on the observation rather than on the features. The aim of clustering techniques are to gather observations in homogeneous groups (wrt the features you observed). It can also be very effective to analyze and understand your data to check the characteristics of the sub-groups ; it can also be very insightful for business teams to have such “categories” they can relate with. 6.3.5 Exercise Interpret the results of the T-SNE embedding 6.4 Visualization bonus : dashboards and reports 6.4.1 Shiny and rmarkdown R offers 2 main packages that allow you to create dashboards and reports in HTML format and the latest feature of web technologies without knowing anything about HTML, CSS or other web-specific languages : Rmarkdown is markdown adapted to R, allows you to create standalone documents in either PDF or HTML format, and mixes regular word-processor features and R code. The flexdashboard package is an extension to Rmarkdown that helps to generate dashboards in HTML format. Those tools are essentially used to design reproducible documents (scientific reports, regularly updated reports/dashboards…). You can also create presentation with it. Shiny is a package to develop interactive web-apps. A shiny app requires an R engine to be rendered on the client side. In the course material, you’ll find one example for each package. You’ll easily find online material to develop your skills further with these tools For rmarkdown For shiny 6.4.2 Web-based graphics with plotly plotly is a graphical library that generate “interactive” web-based graphics. It is available for R, but also python and other programming languages. You can learn the syntax of this package too, but there is a very useful function, plotly::ggplotly() that translates your ggplot object into a plotly graphic. It works for most of the cases. # install.packages(&quot;plotly&quot;) require(plotly) # ggplotly(gg1) This is very useful to embed in a rmarkdown document or a shiny app ! 6.4.3 Other packages &amp; widgets You can use a lot of other html widgets to incorporate in your report/shiny app Example of leaflet : # filter(dat,is_bike) %&gt;% # leaflet::leaflet(data=.) %&gt;% # leaflet::addMarkers(lng = ~startLongitude,lat = ~startLatitude, # popup = ~activityId) %&gt;% # leaflet::addTiles() "],["reg.html", "Chapter 7 Linear and logistic regression 7.1 Linear regression 7.2 Logitic regression", " Chapter 7 Linear and logistic regression Regression is the first machine learning algorithm. It allows you to model a target variable \\(y\\) depending on a set of explanatory variables or features \\(X\\) such that \\(y=f(X) + \\epsilon\\) where \\(f\\) is a linear function (for linear regression). 7.1 Linear regression We will jump directly to the multiple regression model, which is the generalization of the simple linear model, which you can check here 7.1.1 General presentation The basic equation of the linear regression is \\[ y_i = x_i \\cdot b + \\epsilon_i \\Leftrightarrow y_i = \\sum_{j=1}^p x_{ij} b_j + \\epsilon_i\\] Where : \\(x_i\\) is a row-vector of size p (number of explanatory variables), containng the values of each feature of observation i. It is the row i of the matrix \\(X = (x_{ij})\\) b is a column-vector of coefficients, one per explanatory variable \\(\\epsilon_i\\) is the error term for observation i This regression is said to be linear because it is linear in the parameters, you can however transform the original variables at will with non-linear functions (see feature engineering). The biggest assumptions of this model are : Observations are iid There is no perfect multi-collinearity among features \\(\\epsilon_i\\) has a zero conditional mean \\(\\mathbb{E}(\\epsilon | X)=0\\) This last condition helps us to derive an estimator for b (which can be derived in several ways) which is called the OLS estimator (Ordinary Least Squares), which is the solution of the optimization program : \\[\\hat{b}=argmin_b \\sum_{i=1}^n \\epsilon_i^2 = argmin_b \\sum_{i=1}^n (y_i-x_i \\cdot b)^2\\] The solution is \\(\\hat{b} = (X&#39;X)^{-1}X&#39;y\\) where \\(X&#39;=t(X)\\). \\((X&#39;X)^{-1}X\\) is the projection matrix over the hyperplane defined by the features. 7.1.2 Implementation and diagnostics To implement a linear regression with R, we use the lm function : reg &lt;- lm(avgSpeed~avgPower + avgBikeCadence + distance + avgHr + max20MinPower, data=dat_bike) summary(reg) ## ## Call: ## lm(formula = avgSpeed ~ avgPower + avgBikeCadence + distance + ## avgHr + max20MinPower, data = dat_bike) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.858 -4.934 -0.770 4.946 23.774 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 108.95343 4.94740 22.022 &lt;2e-16 *** ## avgPower 0.29912 0.01782 16.789 &lt;2e-16 *** ## avgBikeCadence -1.41070 0.04489 -31.428 &lt;2e-16 *** ## distance 0.11346 0.00917 12.373 &lt;2e-16 *** ## avgHr -0.01114 0.02210 -0.504 0.614 ## max20MinPower -0.12324 0.01369 -9.005 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.425 on 920 degrees of freedom ## (1909 observations deleted due to missingness) ## Multiple R-squared: 0.7813, Adjusted R-squared: 0.7802 ## F-statistic: 657.5 on 5 and 920 DF, p-value: &lt; 2.2e-16 The goodness of fit is measured through 2 main statistics : Adjusted R-squared, \\(1- \\dfrac{n-1}{n-k-1} \\dfrac{SSR}{TSS} \\in [0,1]\\), which takes the number of regressors into account. The closer to 1, the better the fit RMSE (root mean squared error), or residual standard error which has to be compared to the average value of \\(y\\). the smaller the value, the better the fit. 7.1.3 Coefficients interpretation and inference Back to original equation, we can understand how much each feature influences in average the output. \\[ \\dfrac{\\partial y}{\\partial x_1} = b_1\\] Meaning that the increase of \\(x_1\\) by one unit causes the output to increase in average by \\(b_1\\) (which can of course be negative). In our example, one additional watt will result in an increase of the average speed by 0.25 km/h The fundamental hypothesis being fulfilled and the sample being large enough, the distribution of the OLS estimate \\((b_1,...,b_p)\\) are jointly normally distributed, meaning that each \\(\\hat{b_j} \\hookrightarrow \\mathcal{N}(b_j,\\sigma_{b_j}^2)\\) We can therefore perform statistical tests following the previous methodology (see @ref(stat_inf). The most common test is the Student test which tests the null hypothesis \\(b_i=0\\). This allows to check whether a regressor has a significant effect on the target variable or not. But you have to check your residuals ! residuals(reg) %&gt;% data.frame(res=.) %&gt;% ggplot(aes(res)) + geom_density() + theme_minimal() Those are pretty long tailed, which might reflect some outliers or a wrong functional specification ! 7.1.4 The Frisch–Waugh Theorem and the omitted variable bias The Frisch-Waugh theorem tells us that adding a variable as regressor ensures that our estimates controls for the effect of this variable. In other words, you can interpret the coefficients’ values ceteris paribus (other things equal). This also means that if you omit a variable, the coefficient of the other variables are likely to be biased, because you did not take an important variable into account. Back to our example, we can add the elevationGain variable and check what happens : reg &lt;- lm(avgSpeed~avgPower + avgBikeCadence + distance + avgHr + max20MinPower + elevationGain, data=dat_bike) summary(reg) ## ## Call: ## lm(formula = avgSpeed ~ avgPower + avgBikeCadence + distance + ## avgHr + max20MinPower + elevationGain, data = dat_bike) ## ## Residuals: ## Min 1Q Median 3Q Max ## -29.0440 -4.1401 -0.2226 3.6978 19.4709 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.145e+02 4.572e+00 25.032 &lt; 2e-16 *** ## avgPower 1.762e-01 1.683e-02 10.468 &lt; 2e-16 *** ## avgBikeCadence -1.396e+00 4.485e-02 -31.132 &lt; 2e-16 *** ## distance 2.041e-01 1.002e-02 20.364 &lt; 2e-16 *** ## avgHr 1.299e-02 1.916e-02 0.678 0.49799 ## max20MinPower -3.758e-02 1.297e-02 -2.898 0.00386 ** ## elevationGain -1.358e-02 7.626e-04 -17.805 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.283 on 754 degrees of freedom ## (2074 observations deleted due to missingness) ## Multiple R-squared: 0.7441, Adjusted R-squared: 0.742 ## F-statistic: 365.3 on 6 and 754 DF, p-value: &lt; 2.2e-16 See how the coefficients changed. This is understandable because when climbing mountains : More power will not increase the speed, just maintain it (… or not) The cadence is harder to maintain unless you have unlimited gears ! The surprising negative effect of the max20MinPower is no more Notice though that the RMSE and the adjusted \\(R^2\\) degraded… See the variable selection to see how to mitigate that problem. 7.1.5 Feature engineering and functional specification The omitted variable bias makes it very important to include as much variables as possible if you want to be able to estimate the coefficient as accurately as possible. What you can do is add : Exponents to the regressors Interactions between regressors Example with 2 variables \\(y=b_1x_1 + b_2x_2 + b_3x_1^2 + b_4x_1x_2 + \\epsilon\\) In this case : \\(\\dfrac{\\partial y}{\\partial x1} = b_1+2b_3x_1+b_4x_2\\) reg &lt;- lm(avgSpeed~ avgPower + I(avgPower^2) + avgBikeCadence + distance + I(avgPower*distance)+ avgHr + max20MinPower , data=dat_bike) summary(reg) ## ## Call: ## lm(formula = avgSpeed ~ avgPower + I(avgPower^2) + avgBikeCadence + ## distance + I(avgPower * distance) + avgHr + max20MinPower, ## data = dat_bike) ## ## Residuals: ## Min 1Q Median 3Q Max ## -39.079 -4.647 -0.741 4.600 23.527 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 98.6948840 12.6015315 7.832 1.32e-14 *** ## avgPower 0.2597003 0.0974026 2.666 0.00781 ** ## I(avgPower^2) 0.0002079 0.0001771 1.174 0.24056 ## avgBikeCadence -1.3118683 0.0478793 -27.399 &lt; 2e-16 *** ## distance 0.5386573 0.0776404 6.938 7.51e-12 *** ## I(avgPower * distance) -0.0016752 0.0003052 -5.489 5.22e-08 *** ## avgHr -0.0232959 0.0218618 -1.066 0.28689 ## max20MinPower -0.1228978 0.0137991 -8.906 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.309 on 918 degrees of freedom ## (1909 observations deleted due to missingness) ## Multiple R-squared: 0.7886, Adjusted R-squared: 0.787 ## F-statistic: 489.2 on 7 and 918 DF, p-value: &lt; 2.2e-16 reg_full &lt;- lm(avgSpeed~(avgPower + avgBikeCadence + distance + avgHr + max20MinPower + elevationGain)^2, data=dat_bike) summary(reg_full) ## ## Call: ## lm(formula = avgSpeed ~ (avgPower + avgBikeCadence + distance + ## avgHr + max20MinPower + elevationGain)^2, data = dat_bike) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.5744 -2.3189 -0.2251 2.0763 20.4421 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.961e+02 3.265e+01 6.005 3.00e-09 *** ## avgPower 1.553e+00 2.211e-01 7.024 4.89e-12 *** ## avgBikeCadence -2.072e+00 3.737e-01 -5.545 4.09e-08 *** ## distance -4.109e-01 1.872e-01 -2.196 0.0284 * ## avgHr -1.244e-01 2.728e-01 -0.456 0.6485 ## max20MinPower -1.518e+00 2.015e-01 -7.532 1.46e-13 *** ## elevationGain -7.480e-02 1.452e-02 -5.152 3.32e-07 *** ## avgPower:avgBikeCadence -1.382e-02 2.412e-03 -5.730 1.47e-08 *** ## avgPower:distance 8.225e-04 6.085e-04 1.352 0.1769 ## avgPower:avgHr -7.919e-04 1.134e-03 -0.699 0.4850 ## avgPower:max20MinPower 3.357e-05 1.976e-04 0.170 0.8651 ## avgPower:elevationGain -3.885e-04 4.514e-05 -8.606 &lt; 2e-16 *** ## avgBikeCadence:distance 1.380e-02 2.071e-03 6.663 5.25e-11 *** ## avgBikeCadence:avgHr -9.734e-04 2.916e-03 -0.334 0.7386 ## avgBikeCadence:max20MinPower 1.408e-02 1.927e-03 7.305 7.21e-13 *** ## avgBikeCadence:elevationGain 6.942e-04 1.302e-04 5.330 1.30e-07 *** ## distance:avgHr -1.022e-03 6.800e-04 -1.503 0.1332 ## distance:max20MinPower -2.629e-03 4.449e-04 -5.910 5.22e-09 *** ## distance:elevationGain 2.655e-05 1.050e-05 2.529 0.0117 * ## avgHr:max20MinPower 1.664e-03 1.051e-03 1.583 0.1140 ## avgHr:elevationGain 1.373e-04 7.622e-05 1.802 0.0720 . ## max20MinPower:elevationGain 2.854e-04 2.685e-05 10.626 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.606 on 739 degrees of freedom ## (2074 observations deleted due to missingness) ## Multiple R-squared: 0.8652, Adjusted R-squared: 0.8614 ## F-statistic: 225.9 on 21 and 739 DF, p-value: &lt; 2.2e-16 7.1.6 Variable selection So far we focused on getting the best coefficient estimates to be able to interpret how features impact our target variable (“explainable AI”), but following the previous logic, adding the more feature the better ! However, when focusing on the best prediction, you are more interested in finding the most general model which will perform well out of sample adding more and more variables can lead, as a matter of fact, to an overfitted model, which will hardly generalize. This is illustration of the bias-variance trade-off which you will see more in depth during the machine learning session. Regarding regression, avoiding overfitting can be done with variable selection : starting from an extensive model, the procedure will try every feature combination that leads to the best prediction. There are 3 ways of constructing the models : backward selection : remove the less useful feature at a time forward selection : introduce the most useful feature at a time stepwise selection : a mixture of the previous methods The quality of each model is determined by the AIC or BIC which are a function of the opposite of the log-likelihood (because OLS can also be estimated with MLE) and the number of parameters. The lower this number, the better the model. We can implement this method easily selection &lt;- step(reg_full) ## Start: AIC=2346.23 ## avgSpeed ~ (avgPower + avgBikeCadence + distance + avgHr + max20MinPower + ## elevationGain)^2 ## ## Df Sum of Sq RSS AIC ## - avgPower:max20MinPower 1 0.61 15677 2344.3 ## - avgBikeCadence:avgHr 1 2.36 15678 2344.3 ## - avgPower:avgHr 1 10.35 15686 2344.7 ## - avgPower:distance 1 38.75 15715 2346.1 ## &lt;none&gt; 15676 2346.2 ## - distance:avgHr 1 47.93 15724 2346.6 ## - avgHr:max20MinPower 1 53.13 15729 2346.8 ## - avgHr:elevationGain 1 68.85 15745 2347.6 ## - distance:elevationGain 1 135.64 15812 2350.8 ## - avgBikeCadence:elevationGain 1 602.68 16279 2372.9 ## - avgPower:avgBikeCadence 1 696.37 16372 2377.3 ## - distance:max20MinPower 1 740.94 16417 2379.4 ## - avgBikeCadence:distance 1 941.73 16618 2388.6 ## - avgBikeCadence:max20MinPower 1 1131.87 16808 2397.3 ## - avgPower:elevationGain 1 1570.92 17247 2416.9 ## - max20MinPower:elevationGain 1 2395.30 18072 2452.4 ## ## Step: AIC=2344.26 ## avgSpeed ~ avgPower + avgBikeCadence + distance + avgHr + max20MinPower + ## elevationGain + avgPower:avgBikeCadence + avgPower:distance + ## avgPower:avgHr + avgPower:elevationGain + avgBikeCadence:distance + ## avgBikeCadence:avgHr + avgBikeCadence:max20MinPower + avgBikeCadence:elevationGain + ## distance:avgHr + distance:max20MinPower + distance:elevationGain + ## avgHr:max20MinPower + avgHr:elevationGain + max20MinPower:elevationGain ## ## Df Sum of Sq RSS AIC ## - avgBikeCadence:avgHr 1 2.45 15679 2342.4 ## - avgPower:avgHr 1 9.74 15686 2342.7 ## - avgPower:distance 1 38.31 15715 2344.1 ## &lt;none&gt; 15677 2344.3 ## - distance:avgHr 1 49.74 15726 2344.7 ## - avgHr:max20MinPower 1 56.24 15733 2345.0 ## - avgHr:elevationGain 1 68.51 15745 2345.6 ## - distance:elevationGain 1 136.21 15813 2348.8 ## - avgBikeCadence:elevationGain 1 603.23 16280 2371.0 ## - avgPower:avgBikeCadence 1 698.23 16375 2375.4 ## - distance:max20MinPower 1 764.90 16442 2378.5 ## - avgBikeCadence:distance 1 947.00 16624 2386.9 ## - avgBikeCadence:max20MinPower 1 1134.60 16811 2395.4 ## - avgPower:elevationGain 1 1615.03 17292 2416.9 ## - max20MinPower:elevationGain 1 2404.88 18082 2450.9 ## ## Step: AIC=2342.37 ## avgSpeed ~ avgPower + avgBikeCadence + distance + avgHr + max20MinPower + ## elevationGain + avgPower:avgBikeCadence + avgPower:distance + ## avgPower:avgHr + avgPower:elevationGain + avgBikeCadence:distance + ## avgBikeCadence:max20MinPower + avgBikeCadence:elevationGain + ## distance:avgHr + distance:max20MinPower + distance:elevationGain + ## avgHr:max20MinPower + avgHr:elevationGain + max20MinPower:elevationGain ## ## Df Sum of Sq RSS AIC ## - avgPower:avgHr 1 9.62 15689 2340.8 ## - avgPower:distance 1 37.59 15717 2342.2 ## &lt;none&gt; 15679 2342.4 ## - distance:avgHr 1 52.05 15731 2342.9 ## - avgHr:max20MinPower 1 55.42 15735 2343.1 ## - avgHr:elevationGain 1 76.51 15756 2344.1 ## - distance:elevationGain 1 135.62 15815 2346.9 ## - avgBikeCadence:elevationGain 1 610.23 16290 2369.4 ## - avgPower:avgBikeCadence 1 755.98 16435 2376.2 ## - distance:max20MinPower 1 763.12 16442 2376.5 ## - avgBikeCadence:distance 1 961.69 16641 2385.7 ## - avgBikeCadence:max20MinPower 1 1144.49 16824 2394.0 ## - avgPower:elevationGain 1 1638.36 17318 2416.0 ## - max20MinPower:elevationGain 1 2403.13 18082 2448.9 ## ## Step: AIC=2340.84 ## avgSpeed ~ avgPower + avgBikeCadence + distance + avgHr + max20MinPower + ## elevationGain + avgPower:avgBikeCadence + avgPower:distance + ## avgPower:elevationGain + avgBikeCadence:distance + avgBikeCadence:max20MinPower + ## avgBikeCadence:elevationGain + distance:avgHr + distance:max20MinPower + ## distance:elevationGain + avgHr:max20MinPower + avgHr:elevationGain + ## max20MinPower:elevationGain ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 15689 2340.8 ## - avgPower:distance 1 43.52 15732 2340.9 ## - distance:avgHr 1 64.87 15754 2342.0 ## - distance:elevationGain 1 130.08 15819 2345.1 ## - avgHr:elevationGain 1 139.64 15828 2345.6 ## - avgHr:max20MinPower 1 231.50 15920 2350.0 ## - avgBikeCadence:elevationGain 1 611.18 16300 2367.9 ## - distance:max20MinPower 1 760.18 16449 2374.8 ## - avgPower:avgBikeCadence 1 928.82 16618 2382.6 ## - avgBikeCadence:distance 1 959.00 16648 2384.0 ## - avgBikeCadence:max20MinPower 1 1293.15 16982 2399.1 ## - avgPower:elevationGain 1 1756.77 17446 2419.6 ## - max20MinPower:elevationGain 1 2484.52 18173 2450.7 summary(selection) ## ## Call: ## lm(formula = avgSpeed ~ avgPower + avgBikeCadence + distance + ## avgHr + max20MinPower + elevationGain + avgPower:avgBikeCadence + ## avgPower:distance + avgPower:elevationGain + avgBikeCadence:distance + ## avgBikeCadence:max20MinPower + avgBikeCadence:elevationGain + ## distance:avgHr + distance:max20MinPower + distance:elevationGain + ## avgHr:max20MinPower + avgHr:elevationGain + max20MinPower:elevationGain, ## data = dat_bike) ## ## Residuals: ## Min 1Q Median 3Q Max ## -19.7174 -2.3064 -0.2534 2.1279 20.3154 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.999e+02 2.767e+01 7.224 1.26e-12 *** ## avgPower 1.512e+00 1.975e-01 7.654 6.05e-14 *** ## avgBikeCadence -2.101e+00 3.041e-01 -6.909 1.05e-11 *** ## distance -4.161e-01 1.858e-01 -2.239 0.025442 * ## avgHr -2.298e-01 7.599e-02 -3.024 0.002580 ** ## max20MinPower -1.439e+00 1.725e-01 -8.345 3.46e-16 *** ## elevationGain -7.806e-02 1.384e-02 -5.642 2.39e-08 *** ## avgPower:avgBikeCadence -1.451e-02 2.190e-03 -6.628 6.55e-11 *** ## avgPower:distance 8.617e-04 6.006e-04 1.435 0.151793 ## avgPower:elevationGain -3.943e-04 4.325e-05 -9.115 &lt; 2e-16 *** ## avgBikeCadence:distance 1.386e-02 2.058e-03 6.735 3.29e-11 *** ## avgBikeCadence:max20MinPower 1.430e-02 1.828e-03 7.820 1.81e-14 *** ## avgBikeCadence:elevationGain 6.977e-04 1.298e-04 5.376 1.02e-07 *** ## distance:avgHr -1.151e-03 6.570e-04 -1.752 0.080266 . ## distance:max20MinPower -2.595e-03 4.328e-04 -5.996 3.16e-09 *** ## distance:elevationGain 2.587e-05 1.043e-05 2.480 0.013345 * ## avgHr:max20MinPower 1.010e-03 3.051e-04 3.309 0.000982 *** ## avgHr:elevationGain 1.648e-04 6.412e-05 2.570 0.010367 * ## max20MinPower:elevationGain 2.872e-04 2.649e-05 10.840 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.598 on 742 degrees of freedom ## (2074 observations deleted due to missingness) ## Multiple R-squared: 0.8651, Adjusted R-squared: 0.8618 ## F-statistic: 264.4 on 18 and 742 DF, p-value: &lt; 2.2e-16 7.1.7 Exercises Design a regression model that will predict best the theoretical average speed for indoor bike activities (that have no speed, no coordinates…). Feature importance can be very important. Example : to prepare for competitions (that take place between may-september), I always follow a structured training plan at some point. This has a direct impact on the performances. Can you identify when this preparation starts and how to integrate it in the model ? This is somehow connected to your assignment ;) Can you identify the measurement errors (thanks to residuals) From the last functional specification used, design a graphic that shows the final impact of an increase in power to the average speed, taking the distance into account. 7.2 Logitic regression 7.2.1 Mathematical formulation Logistic regression aims to model a binary output. In this case, \\(y \\in \\{0,1\\}\\) and the previous specification can’t apply. We still have a linear relationship, but which applies to the log-odd ratio : \\[log \\dfrac{\\mathbb{P}(y=1|x)}{1-\\mathbb{P}(y=1|x)} = log \\dfrac{^p}{1-p} = x_ib + \\epsilon_i\\] This is called the link function and working the expression further we find that : \\(p(x_i;b) = \\mathbb{P}(y_i=1|x_i) = \\dfrac{1}{1+e^{-x_ib}}\\) This allows us to derive the likelihood : \\[\\mathcal{L}(b) = \\prod_{i=1}^n p(x_i;b)^{y_i} \\cdot (1-p(x_i;b))^{1-y_i}\\] This expression can be simplified, but there is no exact expression as for the OLS \\(\\rightarrow\\) the optimal solution has to be found via numerical optimization (eg Newton-Raphson). 7.2.2 Implementation in R and interpretation In R, we use the glm function while specifying the family. We model the probability of an activity to be bike or something else, which is a binary variable. logit &lt;- glm(is_bike~distance+duration+elevationGain+avgSpeed+avgHr,data=dat_clean,family = &quot;binomial&quot;) summary(logit) ## ## Call: ## glm(formula = is_bike ~ distance + duration + elevationGain + ## avgSpeed + avgHr, family = &quot;binomial&quot;, data = dat_clean) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.5243 -0.7947 0.1971 0.5615 3.0610 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.592e+00 3.531e-01 13.006 &lt; 2e-16 *** ## distance 2.096e-02 6.167e-03 3.398 0.000679 *** ## duration 3.971e-03 2.308e-03 1.720 0.085343 . ## elevationGain -7.682e-05 3.781e-05 -2.031 0.042206 * ## avgSpeed 9.066e-02 8.251e-03 10.988 &lt; 2e-16 *** ## avgHr -4.760e-02 2.445e-03 -19.468 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 5287.9 on 3819 degrees of freedom ## Residual deviance: 3466.6 on 3814 degrees of freedom ## (1980 observations deleted due to missingness) ## AIC: 3478.6 ## ## Number of Fisher Scoring iterations: 5 The sign of the coefficient indicates whether the feature increases the probability for an activity to be a ride ride or not. However, the values cannot be interpreted as directly as in the case of the linear regression. But you can use the exponent of the value of the coefficient and interpret it in terms of odd-ratios. For instance, adding one more kilometer to the average distance multiplies the probability for an activity to be a ride rather than anything else by 1.0211775, meaning 4% more chances. In the contrary, an activity that has 1 bpm more than the average HR has 4.6483832 6% less chances to be a ride. This makes sense because, as observed earlier, rides are longer and the heart rate is a bit smaller than for other activities. 7.2.3 Goodness of fit As you might have noticed, there is no \\(R^2\\) or RMSE in our case, just the AIC (which only allows you to compare different models, not know how good the model is). What we can do is check the fitted values of the model and the actual values pred &lt;- predict(logit,dat_clean,type=&quot;response&quot;) pred_bin &lt;- as.numeric(pred&gt;.5) table(pred_bin,dat_clean$is_bike) ## ## pred_bin FALSE TRUE ## 0 1688 404 ## 1 136 1592 And we can compute the accuracy as the sum of correct predictions divided by total number of activities : 0.8296412 You can derive other goodness of fit metrics from the previous confusion matrix : Sensitivity (recall) : \\(\\dfrac{TP}{TP+FN}\\) Specificity : \\(\\dfrac{TN}{TN+FP}\\) Precision : \\(\\dfrac{TP}{TP+FP}\\) Depending on your business use case, you will focus more on one or the other metric. You will cover this in more detail durinng the machine learning week :) 7.2.4 Exercises Fit a model for avgSpeed with all activities (including the activity type) Fit a model to guess whether an activity is a run or something else "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
